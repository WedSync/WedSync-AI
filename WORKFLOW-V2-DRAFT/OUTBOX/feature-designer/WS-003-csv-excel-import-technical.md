# TECHNICAL SPECIFICATION: WS-003 - CSV/Excel Import
## Generated by Feature Development Session - 2025-08-21

### USER STORY & BUSINESS CONTEXT (THINK HARD - BE FACTUAL)
**As a:** Wedding photographer transitioning from spreadsheets
**I want to:** Import my existing client list from CSV/Excel files with automatic data detection and mapping
**So that:** I can migrate 200+ clients from spreadsheets to WedSync in under 30 minutes instead of spending weeks entering data manually

**Real Wedding Scenario:**
A photographer with 5 years of business has 300 couples across multiple Excel files (2019-bookings.xlsx, 2020-covid-reschedules.csv, 2021-recovery.xlsx). Currently they're afraid to switch platforms because manual re-entry would take 40+ hours. With this feature, they upload each file, the system auto-detects "Couple Names," "Wedding Date," "Venue," columns, maps the data with 95% accuracy, and imports 300 records in 25 minutes - saving 39+ hours of tedious data entry.

### SPECIFICATION SOURCE
- **Feature ID:** WS-003
- **Original Spec:** /CORE-SPECIFICATIONS/02-WEDSYNC-SUPPLIER-PLATFORM/03-Client-Management/04-csv-excel-import md.md
- **Current Implementation:** 0% complete
- **Files to Modify:** None - new feature
- **New Files to Create:**
  - /src/app/(dashboard)/clients/import/page.tsx
  - /src/components/clients/import/FileUploadZone.tsx
  - /src/components/clients/import/DataPreview.tsx
  - /src/components/clients/import/ColumnMapping.tsx
  - /src/components/clients/import/ImportProgress.tsx
  - /src/lib/import/csvParser.ts
  - /src/lib/import/columnDetection.ts
  - /src/lib/import/dataTransformation.ts
  - /src/app/api/import/preview/route.ts
  - /src/app/api/import/execute/route.ts

### TECHNICAL DESIGN

#### Database Schema Required
```sql
-- Import job tracking
CREATE TABLE IF NOT EXISTS import_jobs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  supplier_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
  filename TEXT NOT NULL,
  file_size INTEGER NOT NULL,
  total_rows INTEGER DEFAULT 0,
  processed_rows INTEGER DEFAULT 0,
  successful_rows INTEGER DEFAULT 0,
  failed_rows INTEGER DEFAULT 0,
  status TEXT CHECK (status IN ('uploading', 'previewing', 'mapping', 'processing', 'completed', 'failed')) DEFAULT 'uploading',
  column_mappings JSONB DEFAULT '{}',
  transformation_rules JSONB DEFAULT '{}',
  error_report JSONB DEFAULT '{}',
  preview_data JSONB DEFAULT '{}',
  created_at TIMESTAMP DEFAULT NOW(),
  completed_at TIMESTAMP
);

-- Import field mapping templates
CREATE TABLE IF NOT EXISTS import_field_mappings (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  supplier_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
  template_name TEXT NOT NULL,
  file_type TEXT CHECK (file_type IN ('csv', 'excel', 'google_sheets')),
  column_mappings JSONB NOT NULL,
  detection_patterns JSONB DEFAULT '{}',
  usage_count INTEGER DEFAULT 0,
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);

-- Import validation rules
CREATE TABLE IF NOT EXISTS import_validation_rules (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  field_name TEXT NOT NULL,
  validation_type TEXT CHECK (validation_type IN ('required', 'email', 'phone', 'date', 'regex')),
  validation_pattern TEXT,
  error_message TEXT,
  severity TEXT CHECK (severity IN ('error', 'warning', 'info')) DEFAULT 'error',
  created_at TIMESTAMP DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_import_jobs_supplier_id ON import_jobs(supplier_id);
CREATE INDEX idx_import_jobs_status ON import_jobs(status);
CREATE INDEX idx_import_field_mappings_supplier_id ON import_field_mappings(supplier_id);
```

#### API Endpoints Required
```typescript
// POST /api/import/preview
interface ImportPreviewRequest {
  file: File; // FormData
  fileType: 'csv' | 'excel';
  delimiter?: string; // For CSV
  encoding?: string;
  hasHeaders?: boolean;
}

interface ImportPreviewResponse {
  success: boolean;
  data: {
    jobId: string;
    detectedColumns: Array<{
      index: number;
      name: string;
      sampleValues: string[];
      suggestedMapping: string | null;
      confidence: number; // 0-1
    }>;
    previewRows: Array<Record<string, string>>;
    totalRows: number;
    fileInfo: {
      size: number;
      encoding: string;
      delimiter?: string;
    };
    suggestions: {
      mappings: Record<string, string>;
      transformations: Record<string, any>;
    };
  };
}

// POST /api/import/execute
interface ImportExecuteRequest {
  jobId: string;
  columnMappings: Record<string, string>; // CSV column -> WS field
  transformationRules: {
    nameFormat?: 'first_last' | 'last_first' | 'combined';
    dateFormat?: string;
    phoneFormat?: 'international' | 'national';
    skipEmptyRows?: boolean;
    duplicateHandling?: 'skip' | 'update' | 'create_new';
  };
  validationRules?: {
    requiredFields: string[];
    allowPartialData: boolean;
  };
}

interface ImportExecuteResponse {
  success: boolean;
  data: {
    jobId: string;
    processedCount: number;
    successCount: number;
    errorCount: number;
    errors: Array<{
      row: number;
      field: string;
      value: string;
      error: string;
      severity: 'error' | 'warning';
    }>;
    duplicatesFound: number;
    estimatedCompletionTime: string;
  };
}

// GET /api/import/status/[jobId]
interface ImportStatusResponse {
  success: boolean;
  data: {
    status: 'uploading' | 'previewing' | 'mapping' | 'processing' | 'completed' | 'failed';
    progress: number; // 0-100
    currentRow: number;
    totalRows: number;
    elapsedTime: number; // seconds
    estimatedTimeRemaining: number; // seconds
    errors: Array<any>;
  };
}
```

#### Frontend Components Required
```typescript
// Component: FileUploadZone
// Location: /src/components/clients/import/FileUploadZone.tsx

interface FileUploadZoneProps {
  onFileSelect: (file: File) => void;
  acceptedTypes: string[];
  maxFileSize: number; // MB
  loading: boolean;
}

// Key functionality:
- Drag-and-drop file upload with visual feedback
- File type validation (CSV, XLSX, XLS)
- File size validation (max 10MB)
- Progress indicator during upload
- Error handling for unsupported formats
- Visual previews of file information

// Component: DataPreview
// Location: /src/components/clients/import/DataPreview.tsx

interface DataPreviewProps {
  previewData: Array<Record<string, string>>;
  detectedColumns: Array<ColumnInfo>;
  onColumnMappingChange: (mapping: Record<string, string>) => void;
}

// Key functionality:
- Table view of first 10 rows
- Column header highlighting
- Confidence indicators for auto-detection
- Inline mapping dropdowns
- Data type indicators (text, date, email, phone)
- Sample value tooltips

// Component: ColumnMapping
// Location: /src/components/clients/import/ColumnMapping.tsx

interface ColumnMappingProps {
  sourceColumns: string[];
  targetFields: WedSyncField[];
  mappings: Record<string, string>;
  onMappingChange: (sourceCol: string, targetField: string) => void;
  validationErrors: ValidationError[];
}

// Key functionality:
- Drag-and-drop column to field mapping
- Auto-suggestion based on column names
- Required field indicators
- Validation error display
- Save mapping as template option
- Load from saved templates
```

#### Integration Points
```typescript
// Library: csvParser
// Dependencies: Papa Parse, xlsx, file-saver

import Papa from 'papaparse';
import * as XLSX from 'xlsx';

interface CSVParseOptions {
  delimiter?: string;
  encoding?: string;
  skipEmptyLines: boolean;
  header: boolean;
  preview?: number; // rows to preview
}

export async function parseCSVFile(file: File, options: CSVParseOptions) {
  return new Promise((resolve, reject) => {
    Papa.parse(file, {
      ...options,
      complete: (results) => {
        resolve({
          data: results.data,
          errors: results.errors,
          meta: results.meta
        });
      },
      error: reject,
      // Use web worker for large files
      worker: file.size > 1024 * 1024 // 1MB
    });
  });
}

// Column detection algorithms
export function detectColumnType(values: string[]): ColumnType {
  const patterns = {
    email: /^[^\s@]+@[^\s@]+\.[^\s@]+$/,
    phone: /^[\+]?[1-9][\d]{0,15}$/,
    date: /^\d{1,2}[\/\-]\d{1,2}[\/\-]\d{2,4}$/,
    currency: /^\$?\d+\.?\d{0,2}$/
  };
  
  // Analyze sample values for pattern matching
  const emailCount = values.filter(v => patterns.email.test(v)).length;
  const phoneCount = values.filter(v => patterns.phone.test(v)).length;
  const dateCount = values.filter(v => patterns.date.test(v)).length;
  
  if (emailCount > values.length * 0.7) return 'email';
  if (phoneCount > values.length * 0.7) return 'phone';
  if (dateCount > values.length * 0.7) return 'date';
  
  return 'text';
}
```

### CODE EXAMPLES

#### Example 1: Smart Column Detection Algorithm
```typescript
// ACTUAL CODE PATTERN TO FOLLOW:
import { similarity } from '@/lib/utils/stringUtils';

export function detectWedSyncMapping(columnName: string, sampleValues: string[]): { 
  field: string | null; 
  confidence: number; 
} {
  // Step 1: Normalize column name
  const normalized = columnName.toLowerCase()
    .replace(/[^a-z0-9]/g, '_')
    .replace(/_+/g, '_');
  
  // Step 2: Define mapping patterns with confidence weights
  const patterns = [
    { field: 'couple_names', patterns: ['couple', 'names', 'client', 'bride_groom'], weight: 0.9 },
    { field: 'wedding_date', patterns: ['wedding', 'date', 'event_date'], weight: 0.9 },
    { field: 'venue_name', patterns: ['venue', 'location', 'site'], weight: 0.8 },
    { field: 'partner1_email', patterns: ['email', 'bride_email', 'contact'], weight: 0.8 },
    { field: 'partner1_phone', patterns: ['phone', 'mobile', 'cell'], weight: 0.7 },
    { field: 'guest_count', patterns: ['guests', 'count', 'number'], weight: 0.7 }
  ];
  
  // Step 3: Calculate similarity scores
  let bestMatch = { field: null, confidence: 0 };
  
  for (const pattern of patterns) {
    for (const patternText of pattern.patterns) {
      const similarityScore = similarity(normalized, patternText);
      const confidence = similarityScore * pattern.weight;
      
      if (confidence > bestMatch.confidence && confidence > 0.6) {
        bestMatch = { field: pattern.field, confidence };
      }
    }
  }
  
  // Step 4: Validate with sample data
  if (bestMatch.field && bestMatch.confidence > 0.7) {
    const validationScore = validateSampleData(bestMatch.field, sampleValues);
    bestMatch.confidence = (bestMatch.confidence + validationScore) / 2;
  }
  
  return bestMatch;
}
```

#### Example 2: Batch Processing with Progress Tracking
```typescript
// ACTUAL CODE PATTERN TO FOLLOW:
import { supabase } from '@/lib/supabase';
import { useImportStore } from '@/lib/stores/importStore';

export async function processImportBatch(
  jobId: string, 
  mappedData: Array<Record<string, any>>,
  batchSize: number = 50
) {
  const { updateProgress } = useImportStore.getState();
  const totalRows = mappedData.length;
  let processedCount = 0;
  let successCount = 0;
  let errors: Array<ImportError> = [];
  
  // Step 1: Process in batches to avoid memory issues
  for (let i = 0; i < totalRows; i += batchSize) {
    const batch = mappedData.slice(i, i + batchSize);
    
    try {
      // Step 2: Transform and validate batch data
      const transformedBatch = await Promise.all(
        batch.map(async (row, index) => {
          try {
            return await transformRowData(row, i + index + 1);
          } catch (error) {
            errors.push({
              row: i + index + 1,
              error: error.message,
              data: row
            });
            return null;
          }
        })
      );
      
      // Step 3: Filter out failed transformations
      const validRows = transformedBatch.filter(row => row !== null);
      
      // Step 4: Bulk insert to database
      if (validRows.length > 0) {
        const { data, error } = await supabase
          .from('client_profiles')
          .upsert(validRows, { 
            onConflict: 'partner1_email',
            ignoreDuplicates: false 
          });
          
        if (error) throw error;
        successCount += validRows.length;
      }
      
      processedCount += batch.length;
      
      // Step 5: Update progress in real-time
      updateProgress(jobId, {
        processedRows: processedCount,
        successfulRows: successCount,
        failedRows: errors.length,
        progress: Math.round((processedCount / totalRows) * 100)
      });
      
      // Step 6: Brief pause to prevent database overload
      await new Promise(resolve => setTimeout(resolve, 100));
      
    } catch (batchError) {
      console.error('Batch processing error:', batchError);
      // Mark entire batch as failed but continue processing
      errors.push({
        row: i,
        error: `Batch error: ${batchError.message}`,
        data: batch
      });
    }
  }
  
  // Step 7: Final status update
  await supabase
    .from('import_jobs')
    .update({
      status: errors.length === totalRows ? 'failed' : 'completed',
      processed_rows: processedCount,
      successful_rows: successCount,
      failed_rows: errors.length,
      error_report: errors,
      completed_at: new Date().toISOString()
    })
    .eq('id', jobId);
    
  return {
    processedCount,
    successCount,
    errorCount: errors.length,
    errors
  };
}
```

### MCP SERVER USAGE

#### Required MCP Servers
- [x] Context7: Load docs for Papa Parse, XLSX library
- [ ] Playwright: Test file upload and import flow
- [x] Filesystem: Access import component directories

#### Context7 Queries Needed
```typescript
await mcp__context7__get-library-docs("/mholt/papaparse", "CSV parsing options", 3000);
await mcp__context7__get-library-docs("/sheetjs/sheetjs", "Excel file reading", 2000);
await mcp__context7__get-library-docs("/vercel/next.js", "file upload handling", 2000);
```

### TEST REQUIREMENTS

#### Unit Tests Required
```typescript
describe('CSV Import System', () => {
  it('should detect column types accurately', () => {
    const emailSamples = ['john@test.com', 'jane@example.org'];
    expect(detectColumnType(emailSamples)).toBe('email');
  });
  
  it('should map columns to WedSync fields', () => {
    const result = detectWedSyncMapping('Couple Names', ['John & Jane']);
    expect(result.field).toBe('couple_names');
    expect(result.confidence).toBeGreaterThan(0.8);
  });
  
  it('should handle malformed CSV data gracefully', () => {
    const malformedCSV = 'name,date\nJohn,invalid-date\n';
    expect(() => parseCSVData(malformedCSV)).not.toThrow();
  });
  
  it('should validate required fields', () => {
    const rowData = { partner1_email: 'invalid-email' };
    const errors = validateRowData(rowData);
    expect(errors).toContain('Invalid email format');
  });
});

describe('Batch Processing', () => {
  it('should process large datasets without memory issues', async () => {
    const largeDataset = Array(1000).fill({}).map((_, i) => ({
      couple_names: `Couple ${i}`,
      wedding_date: '2024-06-15'
    }));
    
    const result = await processImportBatch('test-job', largeDataset);
    expect(result.successCount).toBe(1000);
  });
});
```

#### E2E Tests Required
```typescript
// Using Playwright MCP
test('Complete CSV import flow', async () => {
  await mcp__playwright__browser_navigate({url: '/clients/import'});
  
  // Upload CSV file
  await mcp__playwright__browser_file_upload({
    paths: ['/test-fixtures/sample-clients.csv']
  });
  
  // Verify preview appears
  await mcp__playwright__browser_wait_for({text: 'Preview Data'});
  await mcp__playwright__browser_snapshot();
  
  // Test column mapping
  await mcp__playwright__browser_select_option({
    element: 'Column mapping dropdown',
    ref: '[data-testid="mapping-couple-names"]',
    values: ['couple_names']
  });
  
  // Execute import
  await mcp__playwright__browser_click({
    element: 'Import button',
    ref: '[data-testid="execute-import"]'
  });
  
  // Verify completion
  await mcp__playwright__browser_wait_for({text: 'Import completed successfully'});
});

test('Error handling for invalid data', async () => {
  await mcp__playwright__browser_navigate({url: '/clients/import'});
  
  // Upload file with errors
  await mcp__playwright__browser_file_upload({
    paths: ['/test-fixtures/invalid-clients.csv']
  });
  
  // Check error indicators
  await mcp__playwright__browser_wait_for({text: 'validation errors'});
  
  // Verify error report download
  await mcp__playwright__browser_click({
    element: 'Download errors button',
    ref: '[data-testid="download-errors"]'
  });
});
```

### ACCEPTANCE CRITERIA
- [ ] Supports CSV (all delimiters), Excel (.xlsx, .xls) files up to 10MB
- [ ] Auto-detects column mappings with 85%+ accuracy for standard fields
- [ ] Processes 1000+ records in under 2 minutes
- [ ] Provides real-time progress updates during import
- [ ] Handles duplicate detection and resolution options
- [ ] Generates detailed error reports for invalid data
- [ ] Saves successful mapping templates for reuse
- [ ] Validates email, phone, and date formats automatically
- [ ] Gracefully handles partial data and missing fields
- [ ] Supports undo/rollback for completed imports

### DEPENDENCIES
- Must complete after: WS-001 (Client List Views), WS-002 (Client Profiles)
- Must complete before: WS-004 (Bulk Operations)
- Shares code with: WS-002 (client data models), WS-004 (validation logic)

### ESTIMATED EFFORT
- Team A Frontend: 20 hours
- Team B Backend: 16 hours  
- Team C Integration: 4 hours
- Total: 40 hours