# TECHNICAL SPECIFICATION: WS-125 - FAQ Extraction System
## Generated by Feature Development Session - 2025-01-23

### USER STORY & BUSINESS CONTEXT (THINK HARD - BE FACTUAL)
**As a:** Wedding photographer with an existing website containing 20+ FAQs about my services
**I want to:** Automatically extract and import my website FAQs into WedSync's knowledge system
**So that:** I save 3-4 hours of manual FAQ entry and ensure my couples get consistent answers through the chatbot

**Real Wedding Scenario:**
A photographer has a comprehensive FAQ section on their website covering pricing, deliverables, and timeline questions. Instead of manually copying 25 FAQ entries into WedSync, they enter their website URL and the system automatically extracts, categorizes, and structures their FAQs for use in automated customer responses, saving hours of data entry while ensuring accuracy.

### SPECIFICATION SOURCE
- **Feature ID:** WS-125
- **Original Spec:** /CORE-SPECIFICATIONS/04-AI-INTEGRATION/03-Content-Generation/02-faq-extraction md.md
- **Current Implementation:** 0% complete
- **Files to Modify:** None (new feature)
- **New Files to Create:**
  - /src/lib/ai/faq-extractor.ts
  - /src/app/api/faqs/extract/route.ts
  - /src/app/api/faqs/[id]/review/route.ts
  - /src/components/faqs/ExtractionInterface.tsx
  - /src/components/faqs/ReviewQueue.tsx
  - /src/types/faq-extraction.ts

### TECHNICAL DESIGN

#### Database Schema Required
```sql
-- Extracted FAQs storage
CREATE TABLE IF NOT EXISTS extracted_faqs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  supplier_id UUID REFERENCES user_profiles(id),
  question TEXT NOT NULL,
  answer TEXT NOT NULL,
  category TEXT NOT NULL CHECK (category IN ('pricing', 'booking', 'service', 'logistics', 'general')),
  source_url TEXT NOT NULL,
  extraction_date TIMESTAMP DEFAULT NOW(),
  approved BOOLEAN DEFAULT false,
  reviewed_by UUID REFERENCES user_profiles(id),
  reviewed_at TIMESTAMP,
  confidence_score NUMERIC(3,2) DEFAULT 0.0,
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);

-- Website scraping cache
CREATE TABLE IF NOT EXISTS website_scrape_cache (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  url TEXT UNIQUE NOT NULL,
  content TEXT NOT NULL,
  scraped_at TIMESTAMP DEFAULT NOW(),
  expires_at TIMESTAMP DEFAULT (NOW() + INTERVAL '30 days'),
  robots_txt_allowed BOOLEAN DEFAULT true
);

-- FAQ review queue
CREATE TABLE IF NOT EXISTS faq_review_queue (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  extracted_faq_id UUID REFERENCES extracted_faqs(id),
  status TEXT DEFAULT 'pending' CHECK (status IN ('pending', 'approved', 'rejected', 'edited')),
  admin_notes TEXT,
  created_at TIMESTAMP DEFAULT NOW()
);
```

#### API Endpoints Required
```typescript
// POST /api/faqs/extract
interface ExtractFAQsRequest {
  url: string;
  vendorType: 'photographer' | 'dj' | 'caterer' | 'venue' | 'florist';
  overrideRobots?: boolean; // For testing only
}

interface ExtractFAQsResponse {
  success: boolean;
  data: {
    extractedFAQs: ExtractedFAQ[];
    totalFound: number;
    reviewQueueId: string;
  };
  warnings?: string[];
}

// POST /api/faqs/[id]/review
interface ReviewFAQRequest {
  approved: boolean;
  edits?: {
    question?: string;
    answer?: string;
    category?: string;
  };
  adminNotes?: string;
}

// GET /api/faqs/review-queue
interface ReviewQueueResponse {
  success: boolean;
  data: {
    pendingReviews: ExtractedFAQ[];
    total: number;
  };
}
```

#### Frontend Components Required
```typescript
// Component: FAQExtractionInterface
// Location: /src/components/faqs/ExtractionInterface.tsx

interface FAQExtractionProps {
  vendorType: string;
  onExtractionComplete: (faqs: ExtractedFAQ[]) => void;
}

interface ExtractedFAQ {
  id: string;
  question: string;
  answer: string;
  category: string;
  sourceUrl: string;
  confidenceScore: number;
  approved: boolean;
}

// Key functionality:
- URL validation and robots.txt checking
- Progress indicator for scraping process
- Real-time extraction results display
- Category assignment interface
- Bulk approval/rejection controls
```

#### Integration Points
```typescript
// Service: FAQExtractionService
// Dependencies: Playwright, OpenAI, Supabase, Rate limiting

class FAQExtractionService {
  async extractFromWebsite(url: string, vendorType: string): Promise<ExtractedFAQ[]> {
    // Website scraping with Playwright
    // AI-powered Q&A extraction
    // Wedding context enhancement
    // Automatic categorization
  }
  
  async validateRobotsTxt(url: string): Promise<boolean> {
    // Robots.txt compliance checking
    // Rate limiting enforcement
  }
}
```

### CODE EXAMPLES

#### Example 1: Website Scraping and FAQ Extraction
```typescript
// ACTUAL CODE PATTERN TO FOLLOW:
import { chromium } from 'playwright';
import { OpenAI } from 'openai';
import { supabase } from '@/lib/supabase';

export class FAQExtractor {
  private openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY
  });

  async scrapeWebsite(url: string): Promise<string> {
    // Step 1: Check cache first
    const cached = await this.getCachedContent(url);
    if (cached && cached.expires_at > new Date()) {
      return cached.content;
    }
    
    // Step 2: Validate robots.txt
    const robotsAllowed = await this.validateRobotsTxt(url);
    if (!robotsAllowed) {
      throw new Error('Robots.txt disallows scraping');
    }
    
    // Step 3: Scrape website
    const browser = await chromium.launch({ headless: true });
    const page = await browser.newPage();
    
    try {
      await page.goto(url, { waitUntil: 'networkidle' });
      
      // Find FAQ sections using multiple selectors
      const faqSelectors = [
        '[class*="faq" i]',
        '[id*="faq" i]',
        'h2:has-text("FAQ")',
        'h3:has-text("Frequently Asked")',
        '.accordion',
        '.questions'
      ];
      
      let content = '';
      for (const selector of faqSelectors) {
        const elements = await page.$$(selector);
        for (const el of elements) {
          const text = await el.textContent();
          if (text) content += text + '\n\n';
        }
      }
      
      // Step 4: Cache the content
      await this.cacheContent(url, content);
      
      return content;
    } finally {
      await browser.close();
    }
  }
  
  async extractQA(content: string, vendorType: string): Promise<ExtractedFAQ[]> {
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-3.5-turbo',
      messages: [{
        role: 'system',
        content: `Extract question and answer pairs from website content for a ${vendorType}. 
        Return JSON array with 'question', 'answer', and 'category' fields.
        Categories: pricing, booking, service, logistics, general.
        Add wedding context to questions that don't mention weddings.`
      }, {
        role: 'user',
        content: content
      }],
      response_format: { type: 'json_object' },
      temperature: 0.3
    });
    
    const parsed = JSON.parse(completion.choices[0].message.content);
    return parsed.faqs || [];
  }
  
  private async validateRobotsTxt(url: string): Promise<boolean> {
    try {
      const robotsUrl = new URL('/robots.txt', url).toString();
      const response = await fetch(robotsUrl);
      const robotsTxt = await response.text();
      
      // Simple robots.txt parsing - check for disallow rules
      const lines = robotsTxt.split('\n');
      let currentUserAgent = '';
      
      for (const line of lines) {
        if (line.startsWith('User-agent:')) {
          currentUserAgent = line.split(':')[1].trim();
        }
        if ((currentUserAgent === '*' || currentUserAgent === 'WedSync') 
            && line.startsWith('Disallow:')) {
          const disallowPath = line.split(':')[1].trim();
          if (disallowPath === '/' || url.includes(disallowPath)) {
            return false;
          }
        }
      }
      
      return true;
    } catch {
      return true; // If robots.txt not found, allow scraping
    }
  }
}
```

### MCP SERVER USAGE

#### Required MCP Servers
- [ ] Context7: Load docs for Playwright web scraping
- [ ] Playwright: Test FAQ extraction from sample websites
- [ ] Filesystem: Access cached content and temporary files

#### Context7 Queries Needed
```typescript
await mcp__context7__get-library-docs("/microsoft/playwright", "web scraping", 3000);
await mcp__context7__get-library-docs("/openai/openai-node", "structured output", 2000);
```

### TEST REQUIREMENTS

#### Unit Tests Required
```typescript
describe('FAQExtractor', () => {
  it('should extract FAQs from photographer website', async () => {
    const mockContent = `
      Q: How much do you charge for weddings?
      A: Wedding photography packages start at $2500.
    `;
    
    const faqs = await faqExtractor.extractQA(mockContent, 'photographer');
    expect(faqs).toHaveLength(1);
    expect(faqs[0].category).toBe('pricing');
    expect(faqs[0].question).toContain('wedding');
  });
  
  it('should respect robots.txt disallow rules', async () => {
    const disallowedUrl = 'https://example.com/private';
    const allowed = await faqExtractor.validateRobotsTxt(disallowedUrl);
    expect(allowed).toBe(false);
  });
  
  it('should cache scraped content for 30 days', async () => {
    const url = 'https://example.com';
    const content = 'Sample FAQ content';
    
    await faqExtractor.cacheContent(url, content);
    const cached = await faqExtractor.getCachedContent(url);
    
    expect(cached.content).toBe(content);
    expect(cached.expires_at).toBeAfter(new Date());
  });
});
```

#### E2E Tests Required
```typescript
// Using Playwright MCP
test('FAQ extraction workflow', async () => {
  await mcp__playwright__browser_navigate({url: '/faqs/extract'});
  await mcp__playwright__browser_type({
    element: 'Website URL Input',
    ref: 'input[name="websiteUrl"]',
    text: 'https://testsite.example.com'
  });
  await mcp__playwright__browser_click({element: 'Extract FAQs Button', ref: 'button[type="submit"]'});
  await mcp__playwright__browser_wait_for({text: 'Extraction completed'});
  await mcp__playwright__browser_click({element: 'Review Queue Link', ref: 'a[href="/faqs/review"]'});
  await mcp__playwright__browser_snapshot();
});
```

### ACCEPTANCE CRITERIA
- [ ] Extract FAQs from supplier websites with 90%+ accuracy
- [ ] Respect robots.txt and implement rate limiting (max 1 request/second)
- [ ] Cache scraped content for 30 days to avoid re-scraping
- [ ] Categorize FAQs into 5 categories with 85%+ accuracy
- [ ] Process extraction requests within 60 seconds for typical websites
- [ ] Provide manual review queue for extracted FAQs
- [ ] Support wedding context enhancement for generic questions

### DEPENDENCIES
- Must complete after: None - Can start immediately
- Must complete before: WS-126 (Chatbot Knowledge Base needs FAQ data)
- Shares code with: Content generation and AI processing infrastructure

### ESTIMATED EFFORT
- Team A Frontend: 20 hours (Extraction UI, review queue interface)
- Team B Backend: 18 hours (Playwright integration, AI processing, caching)
- Team C Integration: 14 hours (Website validation, robots.txt compliance, queue management)
- Total: 52 hours