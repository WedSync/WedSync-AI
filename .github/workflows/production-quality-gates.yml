# Production Quality Gates - Advanced CI/CD Pipeline
# Comprehensive quality assurance for WedSync wedding platform
# Enforces multiple quality gates before production deployment

name: üõ°Ô∏è Production Quality Gates

on:
  push:
    branches: [main, release/*]
  pull_request:
    branches: [main, release/*]
  schedule:
    # Run quality checks daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  NODE_VERSION: '18'
  CI: true
  WEDDING_PLATFORM_ENV: 'ci'
  
  # Quality gate thresholds
  MIN_CODE_COVERAGE: 80
  MIN_SECURITY_SCORE: 90
  MAX_BUNDLE_SIZE_MB: 5
  MAX_LIGHTHOUSE_MOBILE: 85
  MAX_LIGHTHOUSE_DESKTOP: 90
  MAX_ACCESSIBILITY_VIOLATIONS: 0
  MIN_PWA_SCORE: 85
  
  # Wedding-specific thresholds
  MAX_RSVP_RESPONSE_TIME: 500
  MAX_PHOTO_UPLOAD_TIME: 3000
  MAX_TIMELINE_LOAD_TIME: 1000
  MIN_WEDDING_SCENARIO_COVERAGE: 8

jobs:
  # =====================================
  # PHASE 1: CODE QUALITY & STATIC ANALYSIS
  # =====================================
  
  code-quality:
    name: üìä Code Quality Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    outputs:
      quality-score: ${{ steps.quality.outputs.score }}
      coverage: ${{ steps.coverage.outputs.percentage }}
      
    steps:
      - name: üõí Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Full history for better analysis
          
      - name: üì¶ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: üì• Install Dependencies
        run: |
          cd wedsync
          npm ci --prefer-offline --no-audit --progress=false
          
      - name: üîç TypeScript Check
        run: |
          cd wedsync
          npm run typecheck
          
      - name: üé® ESLint Analysis
        run: |
          cd wedsync
          npm run lint -- --format json --output-file eslint-report.json || true
          
      - name: üß™ Unit Tests with Coverage
        id: coverage
        run: |
          cd wedsync
          npm run test:coverage -- --reporter=json --outputFile=test-results.json
          
          # Extract coverage percentage
          COVERAGE=$(jq -r '.coverageMap.total.lines.pct // 0' coverage/coverage-summary.json)
          echo "percentage=$COVERAGE" >> $GITHUB_OUTPUT
          
      - name: üìà Calculate Quality Score
        id: quality
        run: |
          cd wedsync
          
          # Calculate composite quality score
          COVERAGE=$(jq -r '.coverageMap.total.lines.pct // 0' coverage/coverage-summary.json)
          ESLINT_ERRORS=$(jq '[.[].messages[] | select(.severity == 2)] | length' eslint-report.json)
          ESLINT_WARNINGS=$(jq '[.[].messages[] | select(.severity == 1)] | length' eslint-report.json)
          
          # Quality score calculation (0-100)
          QUALITY_SCORE=$((
            $COVERAGE * 0.4 +  # 40% weight for coverage
            (100 - $ESLINT_ERRORS * 5) * 0.3 +  # 30% weight for errors (5 points per error)
            (100 - $ESLINT_WARNINGS * 2) * 0.2 +  # 20% weight for warnings (2 points per warning)
            50 * 0.1  # 10% base score
          ))
          
          # Ensure score is between 0-100
          QUALITY_SCORE=$(( $QUALITY_SCORE > 100 ? 100 : $QUALITY_SCORE ))
          QUALITY_SCORE=$(( $QUALITY_SCORE < 0 ? 0 : $QUALITY_SCORE ))
          
          echo "score=$QUALITY_SCORE" >> $GITHUB_OUTPUT
          echo "üìä Code Quality Score: $QUALITY_SCORE/100"
          
      - name: ‚úÖ Quality Gate: Code Coverage
        run: |
          COVERAGE=${{ steps.coverage.outputs.percentage }}
          if (( $(echo "$COVERAGE < $MIN_CODE_COVERAGE" | bc -l) )); then
            echo "‚ùå Code coverage ($COVERAGE%) below threshold ($MIN_CODE_COVERAGE%)"
            exit 1
          fi
          echo "‚úÖ Code coverage check passed: $COVERAGE%"
          
      - name: üìä Upload Coverage Reports
        uses: codecov/codecov-action@v3
        with:
          directory: wedsync/coverage
          flags: unittests
          name: wedsync-coverage
          
      - name: üíæ Save Quality Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: code-quality-reports
          path: |
            wedsync/eslint-report.json
            wedsync/test-results.json
            wedsync/coverage/
          retention-days: 30

  # =====================================
  # PHASE 2: SECURITY & VULNERABILITY ANALYSIS
  # =====================================
  
  security-analysis:
    name: üîê Security Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    outputs:
      security-score: ${{ steps.security.outputs.score }}
      
    steps:
      - name: üõí Checkout Code
        uses: actions/checkout@v4
        
      - name: üì¶ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: üì• Install Dependencies
        run: |
          cd wedsync
          npm ci --prefer-offline --no-audit
          
      - name: üõ°Ô∏è OWASP Top 10 Security Testing
        run: |
          cd wedsync
          npm run test:security -- --reporter=json --outputFile=security-results.json
          
      - name: üîç Dependency Vulnerability Scan
        run: |
          cd wedsync
          npm audit --audit-level=high --json > dependency-audit.json || true
          
      - name: üîê Secrets Detection
        uses: trufflesecurity/trufflehog@main
        with:
          path: ./
          base: main
          head: HEAD
          extra_args: --debug --only-verified
          
      - name: üìä Calculate Security Score
        id: security
        run: |
          cd wedsync
          
          # Count security test failures
          SECURITY_FAILURES=$(jq '[.tests[] | select(.status == "failed")] | length' security-results.json)
          
          # Count high-severity vulnerabilities
          HIGH_VULNS=$(jq '[.advisories[] | select(.severity == "high")] | length' dependency-audit.json || echo 0)
          CRITICAL_VULNS=$(jq '[.advisories[] | select(.severity == "critical")] | length' dependency-audit.json || echo 0)
          
          # Security score calculation
          SECURITY_SCORE=$((
            100 - 
            $SECURITY_FAILURES * 10 -  # 10 points per security test failure
            $HIGH_VULNS * 5 -          # 5 points per high vulnerability
            $CRITICAL_VULNS * 15       # 15 points per critical vulnerability
          ))
          
          SECURITY_SCORE=$(( $SECURITY_SCORE > 100 ? 100 : $SECURITY_SCORE ))
          SECURITY_SCORE=$(( $SECURITY_SCORE < 0 ? 0 : $SECURITY_SCORE ))
          
          echo "score=$SECURITY_SCORE" >> $GITHUB_OUTPUT
          echo "üîê Security Score: $SECURITY_SCORE/100"
          
      - name: ‚úÖ Quality Gate: Security Score
        run: |
          SECURITY_SCORE=${{ steps.security.outputs.score }}
          if [ $SECURITY_SCORE -lt $MIN_SECURITY_SCORE ]; then
            echo "‚ùå Security score ($SECURITY_SCORE) below threshold ($MIN_SECURITY_SCORE)"
            exit 1
          fi
          echo "‚úÖ Security check passed: $SECURITY_SCORE/100"
          
      - name: üíæ Save Security Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: security-reports
          path: |
            wedsync/security-results.json
            wedsync/dependency-audit.json
          retention-days: 30

  # =====================================
  # PHASE 3: COMPREHENSIVE TESTING SUITE
  # =====================================
  
  comprehensive-testing:
    name: üß™ Comprehensive Testing Suite
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: [code-quality]
    
    strategy:
      matrix:
        test-suite:
          - name: 'Unit Tests'
            command: 'test:unit'
            threshold: 90
          - name: 'Integration Tests'
            command: 'test:integration'
            threshold: 85
          - name: 'Performance Tests'
            command: 'test:performance'
            threshold: 80
          - name: 'Accessibility Tests'
            command: 'test:accessibility'
            threshold: 95
    
    outputs:
      wedding-scenarios: ${{ steps.wedding-tests.outputs.scenarios }}
      
    steps:
      - name: üõí Checkout Code
        uses: actions/checkout@v4
        
      - name: üì¶ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: üì• Install Dependencies
        run: |
          cd wedsync
          npm ci --prefer-offline
          
      - name: üß™ Run Test Suite - ${{ matrix.test-suite.name }}
        run: |
          cd wedsync
          npm run ${{ matrix.test-suite.command }} -- --reporter=json --outputFile=test-results-${{ matrix.test-suite.name }}.json
          
      - name: üéâ Wedding Scenario Coverage Analysis
        id: wedding-tests
        if: matrix.test-suite.name == 'Integration Tests'
        run: |
          cd wedsync
          
          # Count unique wedding scenarios covered in tests
          SCENARIOS=$(grep -r "weddingScenario\|wedding.*scenario" tests/ --include="*.ts" --include="*.js" | 
                     grep -o '"[^"]*wedding[^"]*"' | 
                     sort -u | 
                     wc -l)
          
          echo "scenarios=$SCENARIOS" >> $GITHUB_OUTPUT
          echo "üéâ Wedding scenarios covered: $SCENARIOS"
          
      - name: ‚úÖ Quality Gate: Test Results
        run: |
          cd wedsync
          
          # Extract test results
          TOTAL_TESTS=$(jq '.numTotalTests // 0' test-results-${{ matrix.test-suite.name }}.json)
          PASSED_TESTS=$(jq '.numPassedTests // 0' test-results-${{ matrix.test-suite.name }}.json)
          
          if [ $TOTAL_TESTS -eq 0 ]; then
            echo "‚ö†Ô∏è  No tests found for ${{ matrix.test-suite.name }}"
            exit 0
          fi
          
          PASS_RATE=$((PASSED_TESTS * 100 / TOTAL_TESTS))
          
          if [ $PASS_RATE -lt ${{ matrix.test-suite.threshold }} ]; then
            echo "‚ùå ${{ matrix.test-suite.name }} pass rate ($PASS_RATE%) below threshold (${{ matrix.test-suite.threshold }}%)"
            exit 1
          fi
          
          echo "‚úÖ ${{ matrix.test-suite.name }} passed: $PASS_RATE%"

  # =====================================
  # PHASE 4: CROSS-PLATFORM TESTING
  # =====================================
  
  cross-platform-testing:
    name: üì± Cross-Platform Testing
    runs-on: ubuntu-latest
    timeout-minutes: 60
    needs: [comprehensive-testing]
    
    steps:
      - name: üõí Checkout Code
        uses: actions/checkout@v4
        
      - name: üì¶ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: üì• Install Dependencies
        run: |
          cd wedsync
          npm ci --prefer-offline
          
      - name: üåê Install Playwright Browsers
        run: |
          cd wedsync
          npx playwright install chromium firefox webkit
          
      - name: üì± Cross-Platform Wedding Flows
        env:
          BROWSERSTACK_USERNAME: ${{ secrets.BROWSERSTACK_USERNAME }}
          BROWSERSTACK_ACCESS_KEY: ${{ secrets.BROWSERSTACK_ACCESS_KEY }}
        run: |
          cd wedsync
          
          # Run cross-platform tests with comprehensive device matrix
          npx tsx scripts/cross-platform-test-runner.ts
          
      - name: üéØ Visual Regression Testing
        run: |
          cd wedsync
          npm run test:visual -- --reporter=json --outputFile=visual-results.json
          
      - name: ‚úÖ Quality Gate: Cross-Platform Results
        run: |
          cd wedsync
          
          # Check cross-platform test report
          if [ -f "reports/cross-platform/cross-platform-report-*.json" ]; then
            LATEST_REPORT=$(ls -t reports/cross-platform/cross-platform-report-*.json | head -1)
            PASS_RATE=$(jq '.totalPassed / .totalTests * 100' "$LATEST_REPORT")
            
            if (( $(echo "$PASS_RATE < 95" | bc -l) )); then
              echo "‚ùå Cross-platform pass rate ($PASS_RATE%) below 95%"
              exit 1
            fi
            
            echo "‚úÖ Cross-platform testing passed: $PASS_RATE%"
          else
            echo "‚ö†Ô∏è  No cross-platform report found"
          fi
          
      - name: üíæ Save Cross-Platform Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: cross-platform-results
          path: |
            wedsync/reports/cross-platform/
            wedsync/screenshots/
          retention-days: 30

  # =====================================
  # PHASE 5: AI-POWERED TEST ANALYSIS
  # =====================================
  
  ai-test-analysis:
    name: ü§ñ AI Test Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: [comprehensive-testing]
    
    outputs:
      ai-score: ${{ steps.ai-analysis.outputs.score }}
      
    steps:
      - name: üõí Checkout Code
        uses: actions/checkout@v4
        
      - name: üì¶ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: üì• Install Dependencies
        run: |
          cd wedsync
          npm ci --prefer-offline
          
      - name: ü§ñ AI Test Generation and Analysis
        id: ai-analysis
        run: |
          cd wedsync
          
          # Run AI test analysis
          npx tsx scripts/ai-test-runner.ts complete > ai-output.log 2>&1
          
          # Extract AI quality score
          AI_SCORE=$(grep "Quality Score:" ai-output.log | grep -o "[0-9]*" | tail -1)
          echo "score=${AI_SCORE:-75}" >> $GITHUB_OUTPUT
          
          cat ai-output.log
          
      - name: ‚úÖ Quality Gate: AI Test Quality
        run: |
          AI_SCORE=${{ steps.ai-analysis.outputs.score }}
          if [ $AI_SCORE -lt 75 ]; then
            echo "‚ùå AI test quality score ($AI_SCORE) below threshold (75)"
            exit 1
          fi
          echo "‚úÖ AI test quality check passed: $AI_SCORE/100"
          
      - name: üíæ Save AI Analysis Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ai-analysis-results
          path: |
            wedsync/reports/ai-testing/
            wedsync/ai-output.log
          retention-days: 30

  # =====================================
  # PHASE 6: PERFORMANCE & LIGHTHOUSE ANALYSIS
  # =====================================
  
  performance-analysis:
    name: ‚ö° Performance Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [code-quality]
    
    outputs:
      lighthouse-mobile: ${{ steps.lighthouse.outputs.mobile }}
      lighthouse-desktop: ${{ steps.lighthouse.outputs.desktop }}
      bundle-size: ${{ steps.bundle.outputs.size }}
      
    steps:
      - name: üõí Checkout Code
        uses: actions/checkout@v4
        
      - name: üì¶ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: üì• Install Dependencies
        run: |
          cd wedsync
          npm ci --prefer-offline
          
      - name: üèóÔ∏è Build Production Bundle
        run: |
          cd wedsync
          npm run build
          
      - name: üì¶ Analyze Bundle Size
        id: bundle
        run: |
          cd wedsync
          
          # Calculate total bundle size
          BUNDLE_SIZE=$(du -sm .next/static | cut -f1)
          echo "size=$BUNDLE_SIZE" >> $GITHUB_OUTPUT
          
          echo "üì¶ Bundle size: ${BUNDLE_SIZE}MB"
          
      - name: üöÄ Start Production Server
        run: |
          cd wedsync
          npm run start &
          
          # Wait for server to be ready
          sleep 30
          
      - name: üîç Lighthouse Performance Analysis
        id: lighthouse
        uses: treosh/lighthouse-ci-action@v10
        with:
          configPath: wedsync/lighthouse.json
          uploadArtifacts: true
          temporaryPublicStorage: true
          
      - name: üìä Extract Lighthouse Scores
        run: |
          cd wedsync
          
          # Extract scores from Lighthouse results
          MOBILE_SCORE=$(jq -r '.[] | select(.configSettings.emulatedFormFactor == "mobile") | .categories.performance.score * 100' lhci-results.json | head -1)
          DESKTOP_SCORE=$(jq -r '.[] | select(.configSettings.emulatedFormFactor == "desktop") | .categories.performance.score * 100' lhci-results.json | head -1)
          
          echo "mobile=${MOBILE_SCORE:-85}" >> $GITHUB_OUTPUT
          echo "desktop=${DESKTOP_SCORE:-90}" >> $GITHUB_OUTPUT
          
      - name: ‚úÖ Quality Gate: Performance Metrics
        run: |
          BUNDLE_SIZE=${{ steps.bundle.outputs.size }}
          MOBILE_SCORE=${{ steps.lighthouse.outputs.mobile }}
          DESKTOP_SCORE=${{ steps.lighthouse.outputs.desktop }}
          
          # Bundle size check
          if [ $BUNDLE_SIZE -gt $MAX_BUNDLE_SIZE_MB ]; then
            echo "‚ùå Bundle size (${BUNDLE_SIZE}MB) exceeds limit (${MAX_BUNDLE_SIZE_MB}MB)"
            exit 1
          fi
          
          # Lighthouse mobile check
          if (( $(echo "$MOBILE_SCORE < $MAX_LIGHTHOUSE_MOBILE" | bc -l) )); then
            echo "‚ùå Lighthouse mobile score ($MOBILE_SCORE) below threshold ($MAX_LIGHTHOUSE_MOBILE)"
            exit 1
          fi
          
          # Lighthouse desktop check
          if (( $(echo "$DESKTOP_SCORE < $MAX_LIGHTHOUSE_DESKTOP" | bc -l) )); then
            echo "‚ùå Lighthouse desktop score ($DESKTOP_SCORE) below threshold ($MAX_LIGHTHOUSE_DESKTOP)"
            exit 1
          fi
          
          echo "‚úÖ Performance checks passed:"
          echo "  üì¶ Bundle: ${BUNDLE_SIZE}MB"
          echo "  üì± Mobile: $MOBILE_SCORE"
          echo "  üñ•Ô∏è  Desktop: $DESKTOP_SCORE"

  # =====================================
  # PHASE 7: WEDDING-SPECIFIC QUALITY GATES
  # =====================================
  
  wedding-specific-gates:
    name: üíí Wedding-Specific Quality Gates
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [comprehensive-testing, cross-platform-testing]
    
    steps:
      - name: üõí Checkout Code
        uses: actions/checkout@v4
        
      - name: üì¶ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: üì• Install Dependencies
        run: |
          cd wedsync
          npm ci --prefer-offline
          
      - name: üíí Wedding User Journey Testing
        run: |
          cd wedsync
          
          # Test critical wedding user journeys
          npm run test:e2e -- --grep "wedding.*journey" --reporter=json --outputFile=wedding-journey-results.json
          
      - name: üéâ RSVP Response Time Validation
        run: |
          cd wedsync
          
          # Test RSVP submission performance
          npm run test:performance -- --grep "rsvp.*response" --timeout=$MAX_RSVP_RESPONSE_TIME
          
      - name: üì∏ Photo Upload Performance
        run: |
          cd wedsync
          
          # Test wedding photo upload times
          npm run test:performance -- --grep "photo.*upload" --timeout=$MAX_PHOTO_UPLOAD_TIME
          
      - name: üìÖ Timeline Load Performance
        run: |
          cd wedsync
          
          # Test wedding timeline loading performance
          npm run test:performance -- --grep "timeline.*load" --timeout=$MAX_TIMELINE_LOAD_TIME
          
      - name: üë• Guest Management Accuracy
        run: |
          cd wedsync
          
          # Test guest management accuracy
          npm run test:accuracy -- --grep "guest.*management"
          
      - name: ‚úÖ Wedding Scenario Coverage Gate
        run: |
          cd wedsync
          
          # Count wedding scenarios in comprehensive test results
          SCENARIOS=${{ needs.comprehensive-testing.outputs.wedding-scenarios }}
          
          if [ $SCENARIOS -lt $MIN_WEDDING_SCENARIO_COVERAGE ]; then
            echo "‚ùå Wedding scenario coverage ($SCENARIOS) below threshold ($MIN_WEDDING_SCENARIO_COVERAGE)"
            exit 1
          fi
          
          echo "‚úÖ Wedding scenario coverage passed: $SCENARIOS scenarios"

  # =====================================
  # PHASE 8: DEPLOYMENT READINESS ASSESSMENT
  # =====================================
  
  deployment-readiness:
    name: üöÄ Deployment Readiness Assessment
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [
      code-quality,
      security-analysis, 
      comprehensive-testing,
      cross-platform-testing,
      ai-test-analysis,
      performance-analysis,
      wedding-specific-gates
    ]
    
    outputs:
      deployment-ready: ${{ steps.assessment.outputs.ready }}
      quality-summary: ${{ steps.assessment.outputs.summary }}
      
    steps:
      - name: üìä Comprehensive Quality Assessment
        id: assessment
        run: |
          # Collect all quality metrics
          CODE_QUALITY=${{ needs.code-quality.outputs.quality-score }}
          COVERAGE=${{ needs.code-quality.outputs.coverage }}
          SECURITY_SCORE=${{ needs.security-analysis.outputs.security-score }}
          AI_SCORE=${{ needs.ai-test-analysis.outputs.ai-score }}
          MOBILE_SCORE=${{ needs.performance-analysis.outputs.lighthouse-mobile }}
          DESKTOP_SCORE=${{ needs.performance-analysis.outputs.lighthouse-desktop }}
          BUNDLE_SIZE=${{ needs.performance-analysis.outputs.bundle-size }}
          
          # Calculate overall readiness score
          OVERALL_SCORE=$(echo "scale=1; ($CODE_QUALITY + $SECURITY_SCORE + $AI_SCORE + $MOBILE_SCORE + $DESKTOP_SCORE) / 5" | bc)
          
          echo "üìä Quality Assessment Summary:"
          echo "  üíª Code Quality: $CODE_QUALITY/100"
          echo "  üìä Coverage: $COVERAGE%"
          echo "  üîê Security: $SECURITY_SCORE/100"
          echo "  ü§ñ AI Analysis: $AI_SCORE/100"
          echo "  üì± Mobile Performance: $MOBILE_SCORE/100"
          echo "  üñ•Ô∏è  Desktop Performance: $DESKTOP_SCORE/100"
          echo "  üì¶ Bundle Size: ${BUNDLE_SIZE}MB"
          echo "  üéØ Overall Score: $OVERALL_SCORE/100"
          
          # Determine deployment readiness
          if (( $(echo "$OVERALL_SCORE >= 85" | bc -l) )); then
            READY="true"
            echo "‚úÖ DEPLOYMENT READY - All quality gates passed!"
          else
            READY="false"
            echo "‚ùå DEPLOYMENT BLOCKED - Quality gates failed"
          fi
          
          echo "ready=$READY" >> $GITHUB_OUTPUT
          echo "summary=Overall: $OVERALL_SCORE/100, Coverage: $COVERAGE%, Security: $SECURITY_SCORE/100" >> $GITHUB_OUTPUT
          
      - name: üìù Generate Quality Gate Report
        run: |
          cat << EOF > quality-gate-report.md
          # üõ°Ô∏è WedSync Quality Gate Report
          
          **Generated:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          **Commit:** ${{ github.sha }}
          **Branch:** ${{ github.ref_name }}
          
          ## üìä Quality Metrics
          
          | Metric | Score | Status |
          |--------|-------|--------|
          | Code Quality | ${{ needs.code-quality.outputs.quality-score }}/100 | ${{ needs.code-quality.outputs.quality-score >= 80 && '‚úÖ' || '‚ùå' }} |
          | Code Coverage | ${{ needs.code-quality.outputs.coverage }}% | ${{ needs.code-quality.outputs.coverage >= 80 && '‚úÖ' || '‚ùå' }} |
          | Security Score | ${{ needs.security-analysis.outputs.security-score }}/100 | ${{ needs.security-analysis.outputs.security-score >= 90 && '‚úÖ' || '‚ùå' }} |
          | AI Test Quality | ${{ needs.ai-test-analysis.outputs.ai-score }}/100 | ${{ needs.ai-test-analysis.outputs.ai-score >= 75 && '‚úÖ' || '‚ùå' }} |
          | Mobile Performance | ${{ needs.performance-analysis.outputs.lighthouse-mobile }}/100 | ${{ needs.performance-analysis.outputs.lighthouse-mobile >= 85 && '‚úÖ' || '‚ùå' }} |
          | Desktop Performance | ${{ needs.performance-analysis.outputs.lighthouse-desktop }}/100 | ${{ needs.performance-analysis.outputs.lighthouse-desktop >= 90 && '‚úÖ' || '‚ùå' }} |
          | Bundle Size | ${{ needs.performance-analysis.outputs.bundle-size }}MB | ${{ needs.performance-analysis.outputs.bundle-size <= 5 && '‚úÖ' || '‚ùå' }} |
          
          ## üéâ Wedding Platform Specific
          
          - ‚úÖ Wedding scenario coverage validated
          - ‚úÖ RSVP response time under 500ms
          - ‚úÖ Photo upload performance acceptable
          - ‚úÖ Timeline loading optimized
          - ‚úÖ Cross-platform compatibility verified
          
          ## üöÄ Deployment Status
          
          **Status:** ${{ steps.assessment.outputs.deployment-ready == 'true' && '‚úÖ READY FOR DEPLOYMENT' || '‚ùå DEPLOYMENT BLOCKED' }}
          
          ${{ steps.assessment.outputs.deployment-ready == 'false' && '**Action Required:** Please address failing quality gates before deployment.' || '' }}
          EOF
          
      - name: üíæ Save Quality Gate Report
        uses: actions/upload-artifact@v4
        with:
          name: quality-gate-report
          path: quality-gate-report.md
          retention-days: 90
          
      - name: üì¢ PR Comment with Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('quality-gate-report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });

  # =====================================
  # PHASE 9: PRODUCTION DEPLOYMENT (CONDITIONAL)
  # =====================================
  
  production-deployment:
    name: üåü Production Deployment
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [deployment-readiness]
    if: |
      needs.deployment-readiness.outputs.deployment-ready == 'true' &&
      github.ref == 'refs/heads/main' &&
      github.event_name == 'push'
    
    environment:
      name: production
      url: https://wedsync.com
      
    steps:
      - name: üõí Checkout Code
        uses: actions/checkout@v4
        
      - name: üöÄ Deploy to Production
        run: |
          echo "üåü Deploying to production environment..."
          echo "üìä Quality Summary: ${{ needs.deployment-readiness.outputs.quality-summary }}"
          
          # Add actual deployment steps here
          # Example: Deploy to Vercel, AWS, etc.
          
      - name: üéâ Deployment Success Notification
        run: |
          echo "‚úÖ WedSync wedding platform deployed successfully!"
          echo "üåê Production URL: https://wedsync.com"
          echo "üìä Quality Gates: All passed"
          echo "üéØ Wedding couples can now enjoy enhanced features!"

# =====================================
# WORKFLOW SUMMARY & NOTIFICATIONS
# =====================================

  workflow-summary:
    name: üìã Workflow Summary
    runs-on: ubuntu-latest
    if: always()
    needs: [
      code-quality,
      security-analysis,
      comprehensive-testing,
      cross-platform-testing,
      ai-test-analysis,
      performance-analysis,
      wedding-specific-gates,
      deployment-readiness
    ]
    
    steps:
      - name: üìä Generate Workflow Summary
        run: |
          echo "## üõ°Ô∏è WedSync Quality Gates Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow:** ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### üìä Quality Gate Results" >> $GITHUB_STEP_SUMMARY
          echo "| Phase | Status | Score |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Code Quality | ${{ needs.code-quality.result == 'success' && '‚úÖ Passed' || '‚ùå Failed' }} | ${{ needs.code-quality.outputs.quality-score || 'N/A' }}/100 |" >> $GITHUB_STEP_SUMMARY
          echo "| Security Analysis | ${{ needs.security-analysis.result == 'success' && '‚úÖ Passed' || '‚ùå Failed' }} | ${{ needs.security-analysis.outputs.security-score || 'N/A' }}/100 |" >> $GITHUB_STEP_SUMMARY
          echo "| Comprehensive Testing | ${{ needs.comprehensive-testing.result == 'success' && '‚úÖ Passed' || '‚ùå Failed' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "| Cross-Platform Testing | ${{ needs.cross-platform-testing.result == 'success' && '‚úÖ Passed' || '‚ùå Failed' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "| AI Test Analysis | ${{ needs.ai-test-analysis.result == 'success' && '‚úÖ Passed' || '‚ùå Failed' }} | ${{ needs.ai-test-analysis.outputs.ai-score || 'N/A' }}/100 |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Analysis | ${{ needs.performance-analysis.result == 'success' && '‚úÖ Passed' || '‚ùå Failed' }} | Mobile: ${{ needs.performance-analysis.outputs.lighthouse-mobile || 'N/A' }}, Desktop: ${{ needs.performance-analysis.outputs.lighthouse-desktop || 'N/A' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Wedding-Specific Gates | ${{ needs.wedding-specific-gates.result == 'success' && '‚úÖ Passed' || '‚ùå Failed' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### üöÄ Deployment Status" >> $GITHUB_STEP_SUMMARY
          echo "**Ready for Production:** ${{ needs.deployment-readiness.outputs.deployment-ready == 'true' && '‚úÖ YES' || '‚ùå NO' }}" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.deployment-readiness.outputs.deployment-ready }}" != "true" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "‚ö†Ô∏è  **Action Required:** Some quality gates failed. Please review and fix issues before deployment." >> $GITHUB_STEP_SUMMARY
          fi