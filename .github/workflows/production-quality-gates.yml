# Production Quality Gates - Advanced CI/CD Pipeline
# Comprehensive quality assurance for WedSync wedding platform
# Enforces multiple quality gates before production deployment

name: 🛡️ Production Quality Gates

on:
  push:
    branches: [main, release/*]
  pull_request:
    branches: [main, release/*]
  schedule:
    # Run quality checks daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  NODE_VERSION: '18'
  CI: true
  WEDDING_PLATFORM_ENV: 'ci'
  
  # Quality gate thresholds
  MIN_CODE_COVERAGE: 80
  MIN_SECURITY_SCORE: 90
  MAX_BUNDLE_SIZE_MB: 5
  MAX_LIGHTHOUSE_MOBILE: 85
  MAX_LIGHTHOUSE_DESKTOP: 90
  MAX_ACCESSIBILITY_VIOLATIONS: 0
  MIN_PWA_SCORE: 85
  
  # Wedding-specific thresholds
  MAX_RSVP_RESPONSE_TIME: 500
  MAX_PHOTO_UPLOAD_TIME: 3000
  MAX_TIMELINE_LOAD_TIME: 1000
  MIN_WEDDING_SCENARIO_COVERAGE: 8

jobs:
  # =====================================
  # PHASE 1: CODE QUALITY & STATIC ANALYSIS
  # =====================================
  
  code-quality:
    name: 📊 Code Quality Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    outputs:
      quality-score: ${{ steps.quality.outputs.score }}
      coverage: ${{ steps.coverage.outputs.percentage }}
      
    steps:
      - name: 🛒 Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Full history for better analysis
          
      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: 📥 Install Dependencies
        run: |
          cd wedsync
          npm ci --prefer-offline --no-audit --progress=false
          
      - name: 🔍 TypeScript Check
        run: |
          cd wedsync
          npm run typecheck
          
      - name: 🎨 ESLint Analysis
        run: |
          cd wedsync
          npm run lint -- --format json --output-file eslint-report.json || true
          
      - name: 🧪 Unit Tests with Coverage
        id: coverage
        run: |
          cd wedsync
          npm run test:coverage -- --reporter=json --outputFile=test-results.json
          
          # Extract coverage percentage
          COVERAGE=$(jq -r '.coverageMap.total.lines.pct // 0' coverage/coverage-summary.json)
          echo "percentage=$COVERAGE" >> $GITHUB_OUTPUT
          
      - name: 📈 Calculate Quality Score
        id: quality
        run: |
          cd wedsync
          
          # Calculate composite quality score
          COVERAGE=$(jq -r '.coverageMap.total.lines.pct // 0' coverage/coverage-summary.json)
          ESLINT_ERRORS=$(jq '[.[].messages[] | select(.severity == 2)] | length' eslint-report.json)
          ESLINT_WARNINGS=$(jq '[.[].messages[] | select(.severity == 1)] | length' eslint-report.json)
          
          # Quality score calculation (0-100)
          QUALITY_SCORE=$((
            $COVERAGE * 0.4 +  # 40% weight for coverage
            (100 - $ESLINT_ERRORS * 5) * 0.3 +  # 30% weight for errors (5 points per error)
            (100 - $ESLINT_WARNINGS * 2) * 0.2 +  # 20% weight for warnings (2 points per warning)
            50 * 0.1  # 10% base score
          ))
          
          # Ensure score is between 0-100
          QUALITY_SCORE=$(( $QUALITY_SCORE > 100 ? 100 : $QUALITY_SCORE ))
          QUALITY_SCORE=$(( $QUALITY_SCORE < 0 ? 0 : $QUALITY_SCORE ))
          
          echo "score=$QUALITY_SCORE" >> $GITHUB_OUTPUT
          echo "📊 Code Quality Score: $QUALITY_SCORE/100"
          
      - name: ✅ Quality Gate: Code Coverage
        run: |
          COVERAGE=${{ steps.coverage.outputs.percentage }}
          if (( $(echo "$COVERAGE < $MIN_CODE_COVERAGE" | bc -l) )); then
            echo "❌ Code coverage ($COVERAGE%) below threshold ($MIN_CODE_COVERAGE%)"
            exit 1
          fi
          echo "✅ Code coverage check passed: $COVERAGE%"
          
      - name: 📊 Upload Coverage Reports
        uses: codecov/codecov-action@v3
        with:
          directory: wedsync/coverage
          flags: unittests
          name: wedsync-coverage
          
      - name: 💾 Save Quality Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: code-quality-reports
          path: |
            wedsync/eslint-report.json
            wedsync/test-results.json
            wedsync/coverage/
          retention-days: 30

  # =====================================
  # PHASE 2: SECURITY & VULNERABILITY ANALYSIS
  # =====================================
  
  security-analysis:
    name: 🔐 Security Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    outputs:
      security-score: ${{ steps.security.outputs.score }}
      
    steps:
      - name: 🛒 Checkout Code
        uses: actions/checkout@v4
        
      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: 📥 Install Dependencies
        run: |
          cd wedsync
          npm ci --prefer-offline --no-audit
          
      - name: 🛡️ OWASP Top 10 Security Testing
        run: |
          cd wedsync
          npm run test:security -- --reporter=json --outputFile=security-results.json
          
      - name: 🔍 Dependency Vulnerability Scan
        run: |
          cd wedsync
          npm audit --audit-level=high --json > dependency-audit.json || true
          
      - name: 🔐 Secrets Detection
        uses: trufflesecurity/trufflehog@main
        with:
          path: ./
          base: main
          head: HEAD
          extra_args: --debug --only-verified
          
      - name: 📊 Calculate Security Score
        id: security
        run: |
          cd wedsync
          
          # Count security test failures
          SECURITY_FAILURES=$(jq '[.tests[] | select(.status == "failed")] | length' security-results.json)
          
          # Count high-severity vulnerabilities
          HIGH_VULNS=$(jq '[.advisories[] | select(.severity == "high")] | length' dependency-audit.json || echo 0)
          CRITICAL_VULNS=$(jq '[.advisories[] | select(.severity == "critical")] | length' dependency-audit.json || echo 0)
          
          # Security score calculation
          SECURITY_SCORE=$((
            100 - 
            $SECURITY_FAILURES * 10 -  # 10 points per security test failure
            $HIGH_VULNS * 5 -          # 5 points per high vulnerability
            $CRITICAL_VULNS * 15       # 15 points per critical vulnerability
          ))
          
          SECURITY_SCORE=$(( $SECURITY_SCORE > 100 ? 100 : $SECURITY_SCORE ))
          SECURITY_SCORE=$(( $SECURITY_SCORE < 0 ? 0 : $SECURITY_SCORE ))
          
          echo "score=$SECURITY_SCORE" >> $GITHUB_OUTPUT
          echo "🔐 Security Score: $SECURITY_SCORE/100"
          
      - name: ✅ Quality Gate: Security Score
        run: |
          SECURITY_SCORE=${{ steps.security.outputs.score }}
          if [ $SECURITY_SCORE -lt $MIN_SECURITY_SCORE ]; then
            echo "❌ Security score ($SECURITY_SCORE) below threshold ($MIN_SECURITY_SCORE)"
            exit 1
          fi
          echo "✅ Security check passed: $SECURITY_SCORE/100"
          
      - name: 💾 Save Security Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: security-reports
          path: |
            wedsync/security-results.json
            wedsync/dependency-audit.json
          retention-days: 30

  # =====================================
  # PHASE 3: COMPREHENSIVE TESTING SUITE
  # =====================================
  
  comprehensive-testing:
    name: 🧪 Comprehensive Testing Suite
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: [code-quality]
    
    strategy:
      matrix:
        test-suite:
          - name: 'Unit Tests'
            command: 'test:unit'
            threshold: 90
          - name: 'Integration Tests'
            command: 'test:integration'
            threshold: 85
          - name: 'Performance Tests'
            command: 'test:performance'
            threshold: 80
          - name: 'Accessibility Tests'
            command: 'test:accessibility'
            threshold: 95
    
    outputs:
      wedding-scenarios: ${{ steps.wedding-tests.outputs.scenarios }}
      
    steps:
      - name: 🛒 Checkout Code
        uses: actions/checkout@v4
        
      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: 📥 Install Dependencies
        run: |
          cd wedsync
          npm ci --prefer-offline
          
      - name: 🧪 Run Test Suite - ${{ matrix.test-suite.name }}
        run: |
          cd wedsync
          npm run ${{ matrix.test-suite.command }} -- --reporter=json --outputFile=test-results-${{ matrix.test-suite.name }}.json
          
      - name: 🎉 Wedding Scenario Coverage Analysis
        id: wedding-tests
        if: matrix.test-suite.name == 'Integration Tests'
        run: |
          cd wedsync
          
          # Count unique wedding scenarios covered in tests
          SCENARIOS=$(grep -r "weddingScenario\|wedding.*scenario" tests/ --include="*.ts" --include="*.js" | 
                     grep -o '"[^"]*wedding[^"]*"' | 
                     sort -u | 
                     wc -l)
          
          echo "scenarios=$SCENARIOS" >> $GITHUB_OUTPUT
          echo "🎉 Wedding scenarios covered: $SCENARIOS"
          
      - name: ✅ Quality Gate: Test Results
        run: |
          cd wedsync
          
          # Extract test results
          TOTAL_TESTS=$(jq '.numTotalTests // 0' test-results-${{ matrix.test-suite.name }}.json)
          PASSED_TESTS=$(jq '.numPassedTests // 0' test-results-${{ matrix.test-suite.name }}.json)
          
          if [ $TOTAL_TESTS -eq 0 ]; then
            echo "⚠️  No tests found for ${{ matrix.test-suite.name }}"
            exit 0
          fi
          
          PASS_RATE=$((PASSED_TESTS * 100 / TOTAL_TESTS))
          
          if [ $PASS_RATE -lt ${{ matrix.test-suite.threshold }} ]; then
            echo "❌ ${{ matrix.test-suite.name }} pass rate ($PASS_RATE%) below threshold (${{ matrix.test-suite.threshold }}%)"
            exit 1
          fi
          
          echo "✅ ${{ matrix.test-suite.name }} passed: $PASS_RATE%"

  # =====================================
  # PHASE 4: CROSS-PLATFORM TESTING
  # =====================================
  
  cross-platform-testing:
    name: 📱 Cross-Platform Testing
    runs-on: ubuntu-latest
    timeout-minutes: 60
    needs: [comprehensive-testing]
    
    steps:
      - name: 🛒 Checkout Code
        uses: actions/checkout@v4
        
      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: 📥 Install Dependencies
        run: |
          cd wedsync
          npm ci --prefer-offline
          
      - name: 🌐 Install Playwright Browsers
        run: |
          cd wedsync
          npx playwright install chromium firefox webkit
          
      - name: 📱 Cross-Platform Wedding Flows
        env:
          BROWSERSTACK_USERNAME: ${{ secrets.BROWSERSTACK_USERNAME }}
          BROWSERSTACK_ACCESS_KEY: ${{ secrets.BROWSERSTACK_ACCESS_KEY }}
        run: |
          cd wedsync
          
          # Run cross-platform tests with comprehensive device matrix
          npx tsx scripts/cross-platform-test-runner.ts
          
      - name: 🎯 Visual Regression Testing
        run: |
          cd wedsync
          npm run test:visual -- --reporter=json --outputFile=visual-results.json
          
      - name: ✅ Quality Gate: Cross-Platform Results
        run: |
          cd wedsync
          
          # Check cross-platform test report
          if [ -f "reports/cross-platform/cross-platform-report-*.json" ]; then
            LATEST_REPORT=$(ls -t reports/cross-platform/cross-platform-report-*.json | head -1)
            PASS_RATE=$(jq '.totalPassed / .totalTests * 100' "$LATEST_REPORT")
            
            if (( $(echo "$PASS_RATE < 95" | bc -l) )); then
              echo "❌ Cross-platform pass rate ($PASS_RATE%) below 95%"
              exit 1
            fi
            
            echo "✅ Cross-platform testing passed: $PASS_RATE%"
          else
            echo "⚠️  No cross-platform report found"
          fi
          
      - name: 💾 Save Cross-Platform Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: cross-platform-results
          path: |
            wedsync/reports/cross-platform/
            wedsync/screenshots/
          retention-days: 30

  # =====================================
  # PHASE 5: AI-POWERED TEST ANALYSIS
  # =====================================
  
  ai-test-analysis:
    name: 🤖 AI Test Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: [comprehensive-testing]
    
    outputs:
      ai-score: ${{ steps.ai-analysis.outputs.score }}
      
    steps:
      - name: 🛒 Checkout Code
        uses: actions/checkout@v4
        
      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: 📥 Install Dependencies
        run: |
          cd wedsync
          npm ci --prefer-offline
          
      - name: 🤖 AI Test Generation and Analysis
        id: ai-analysis
        run: |
          cd wedsync
          
          # Run AI test analysis
          npx tsx scripts/ai-test-runner.ts complete > ai-output.log 2>&1
          
          # Extract AI quality score
          AI_SCORE=$(grep "Quality Score:" ai-output.log | grep -o "[0-9]*" | tail -1)
          echo "score=${AI_SCORE:-75}" >> $GITHUB_OUTPUT
          
          cat ai-output.log
          
      - name: ✅ Quality Gate: AI Test Quality
        run: |
          AI_SCORE=${{ steps.ai-analysis.outputs.score }}
          if [ $AI_SCORE -lt 75 ]; then
            echo "❌ AI test quality score ($AI_SCORE) below threshold (75)"
            exit 1
          fi
          echo "✅ AI test quality check passed: $AI_SCORE/100"
          
      - name: 💾 Save AI Analysis Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ai-analysis-results
          path: |
            wedsync/reports/ai-testing/
            wedsync/ai-output.log
          retention-days: 30

  # =====================================
  # PHASE 6: PERFORMANCE & LIGHTHOUSE ANALYSIS
  # =====================================
  
  performance-analysis:
    name: ⚡ Performance Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [code-quality]
    
    outputs:
      lighthouse-mobile: ${{ steps.lighthouse.outputs.mobile }}
      lighthouse-desktop: ${{ steps.lighthouse.outputs.desktop }}
      bundle-size: ${{ steps.bundle.outputs.size }}
      
    steps:
      - name: 🛒 Checkout Code
        uses: actions/checkout@v4
        
      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: 📥 Install Dependencies
        run: |
          cd wedsync
          npm ci --prefer-offline
          
      - name: 🏗️ Build Production Bundle
        run: |
          cd wedsync
          npm run build
          
      - name: 📦 Analyze Bundle Size
        id: bundle
        run: |
          cd wedsync
          
          # Calculate total bundle size
          BUNDLE_SIZE=$(du -sm .next/static | cut -f1)
          echo "size=$BUNDLE_SIZE" >> $GITHUB_OUTPUT
          
          echo "📦 Bundle size: ${BUNDLE_SIZE}MB"
          
      - name: 🚀 Start Production Server
        run: |
          cd wedsync
          npm run start &
          
          # Wait for server to be ready
          sleep 30
          
      - name: 🔍 Lighthouse Performance Analysis
        id: lighthouse
        uses: treosh/lighthouse-ci-action@v10
        with:
          configPath: wedsync/lighthouse.json
          uploadArtifacts: true
          temporaryPublicStorage: true
          
      - name: 📊 Extract Lighthouse Scores
        run: |
          cd wedsync
          
          # Extract scores from Lighthouse results
          MOBILE_SCORE=$(jq -r '.[] | select(.configSettings.emulatedFormFactor == "mobile") | .categories.performance.score * 100' lhci-results.json | head -1)
          DESKTOP_SCORE=$(jq -r '.[] | select(.configSettings.emulatedFormFactor == "desktop") | .categories.performance.score * 100' lhci-results.json | head -1)
          
          echo "mobile=${MOBILE_SCORE:-85}" >> $GITHUB_OUTPUT
          echo "desktop=${DESKTOP_SCORE:-90}" >> $GITHUB_OUTPUT
          
      - name: ✅ Quality Gate: Performance Metrics
        run: |
          BUNDLE_SIZE=${{ steps.bundle.outputs.size }}
          MOBILE_SCORE=${{ steps.lighthouse.outputs.mobile }}
          DESKTOP_SCORE=${{ steps.lighthouse.outputs.desktop }}
          
          # Bundle size check
          if [ $BUNDLE_SIZE -gt $MAX_BUNDLE_SIZE_MB ]; then
            echo "❌ Bundle size (${BUNDLE_SIZE}MB) exceeds limit (${MAX_BUNDLE_SIZE_MB}MB)"
            exit 1
          fi
          
          # Lighthouse mobile check
          if (( $(echo "$MOBILE_SCORE < $MAX_LIGHTHOUSE_MOBILE" | bc -l) )); then
            echo "❌ Lighthouse mobile score ($MOBILE_SCORE) below threshold ($MAX_LIGHTHOUSE_MOBILE)"
            exit 1
          fi
          
          # Lighthouse desktop check
          if (( $(echo "$DESKTOP_SCORE < $MAX_LIGHTHOUSE_DESKTOP" | bc -l) )); then
            echo "❌ Lighthouse desktop score ($DESKTOP_SCORE) below threshold ($MAX_LIGHTHOUSE_DESKTOP)"
            exit 1
          fi
          
          echo "✅ Performance checks passed:"
          echo "  📦 Bundle: ${BUNDLE_SIZE}MB"
          echo "  📱 Mobile: $MOBILE_SCORE"
          echo "  🖥️  Desktop: $DESKTOP_SCORE"

  # =====================================
  # PHASE 7: WEDDING-SPECIFIC QUALITY GATES
  # =====================================
  
  wedding-specific-gates:
    name: 💒 Wedding-Specific Quality Gates
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [comprehensive-testing, cross-platform-testing]
    
    steps:
      - name: 🛒 Checkout Code
        uses: actions/checkout@v4
        
      - name: 📦 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: 📥 Install Dependencies
        run: |
          cd wedsync
          npm ci --prefer-offline
          
      - name: 💒 Wedding User Journey Testing
        run: |
          cd wedsync
          
          # Test critical wedding user journeys
          npm run test:e2e -- --grep "wedding.*journey" --reporter=json --outputFile=wedding-journey-results.json
          
      - name: 🎉 RSVP Response Time Validation
        run: |
          cd wedsync
          
          # Test RSVP submission performance
          npm run test:performance -- --grep "rsvp.*response" --timeout=$MAX_RSVP_RESPONSE_TIME
          
      - name: 📸 Photo Upload Performance
        run: |
          cd wedsync
          
          # Test wedding photo upload times
          npm run test:performance -- --grep "photo.*upload" --timeout=$MAX_PHOTO_UPLOAD_TIME
          
      - name: 📅 Timeline Load Performance
        run: |
          cd wedsync
          
          # Test wedding timeline loading performance
          npm run test:performance -- --grep "timeline.*load" --timeout=$MAX_TIMELINE_LOAD_TIME
          
      - name: 👥 Guest Management Accuracy
        run: |
          cd wedsync
          
          # Test guest management accuracy
          npm run test:accuracy -- --grep "guest.*management"
          
      - name: ✅ Wedding Scenario Coverage Gate
        run: |
          cd wedsync
          
          # Count wedding scenarios in comprehensive test results
          SCENARIOS=${{ needs.comprehensive-testing.outputs.wedding-scenarios }}
          
          if [ $SCENARIOS -lt $MIN_WEDDING_SCENARIO_COVERAGE ]; then
            echo "❌ Wedding scenario coverage ($SCENARIOS) below threshold ($MIN_WEDDING_SCENARIO_COVERAGE)"
            exit 1
          fi
          
          echo "✅ Wedding scenario coverage passed: $SCENARIOS scenarios"

  # =====================================
  # PHASE 8: DEPLOYMENT READINESS ASSESSMENT
  # =====================================
  
  deployment-readiness:
    name: 🚀 Deployment Readiness Assessment
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [
      code-quality,
      security-analysis, 
      comprehensive-testing,
      cross-platform-testing,
      ai-test-analysis,
      performance-analysis,
      wedding-specific-gates
    ]
    
    outputs:
      deployment-ready: ${{ steps.assessment.outputs.ready }}
      quality-summary: ${{ steps.assessment.outputs.summary }}
      
    steps:
      - name: 📊 Comprehensive Quality Assessment
        id: assessment
        run: |
          # Collect all quality metrics
          CODE_QUALITY=${{ needs.code-quality.outputs.quality-score }}
          COVERAGE=${{ needs.code-quality.outputs.coverage }}
          SECURITY_SCORE=${{ needs.security-analysis.outputs.security-score }}
          AI_SCORE=${{ needs.ai-test-analysis.outputs.ai-score }}
          MOBILE_SCORE=${{ needs.performance-analysis.outputs.lighthouse-mobile }}
          DESKTOP_SCORE=${{ needs.performance-analysis.outputs.lighthouse-desktop }}
          BUNDLE_SIZE=${{ needs.performance-analysis.outputs.bundle-size }}
          
          # Calculate overall readiness score
          OVERALL_SCORE=$(echo "scale=1; ($CODE_QUALITY + $SECURITY_SCORE + $AI_SCORE + $MOBILE_SCORE + $DESKTOP_SCORE) / 5" | bc)
          
          echo "📊 Quality Assessment Summary:"
          echo "  💻 Code Quality: $CODE_QUALITY/100"
          echo "  📊 Coverage: $COVERAGE%"
          echo "  🔐 Security: $SECURITY_SCORE/100"
          echo "  🤖 AI Analysis: $AI_SCORE/100"
          echo "  📱 Mobile Performance: $MOBILE_SCORE/100"
          echo "  🖥️  Desktop Performance: $DESKTOP_SCORE/100"
          echo "  📦 Bundle Size: ${BUNDLE_SIZE}MB"
          echo "  🎯 Overall Score: $OVERALL_SCORE/100"
          
          # Determine deployment readiness
          if (( $(echo "$OVERALL_SCORE >= 85" | bc -l) )); then
            READY="true"
            echo "✅ DEPLOYMENT READY - All quality gates passed!"
          else
            READY="false"
            echo "❌ DEPLOYMENT BLOCKED - Quality gates failed"
          fi
          
          echo "ready=$READY" >> $GITHUB_OUTPUT
          echo "summary=Overall: $OVERALL_SCORE/100, Coverage: $COVERAGE%, Security: $SECURITY_SCORE/100" >> $GITHUB_OUTPUT
          
      - name: 📝 Generate Quality Gate Report
        run: |
          cat << EOF > quality-gate-report.md
          # 🛡️ WedSync Quality Gate Report
          
          **Generated:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          **Commit:** ${{ github.sha }}
          **Branch:** ${{ github.ref_name }}
          
          ## 📊 Quality Metrics
          
          | Metric | Score | Status |
          |--------|-------|--------|
          | Code Quality | ${{ needs.code-quality.outputs.quality-score }}/100 | ${{ needs.code-quality.outputs.quality-score >= 80 && '✅' || '❌' }} |
          | Code Coverage | ${{ needs.code-quality.outputs.coverage }}% | ${{ needs.code-quality.outputs.coverage >= 80 && '✅' || '❌' }} |
          | Security Score | ${{ needs.security-analysis.outputs.security-score }}/100 | ${{ needs.security-analysis.outputs.security-score >= 90 && '✅' || '❌' }} |
          | AI Test Quality | ${{ needs.ai-test-analysis.outputs.ai-score }}/100 | ${{ needs.ai-test-analysis.outputs.ai-score >= 75 && '✅' || '❌' }} |
          | Mobile Performance | ${{ needs.performance-analysis.outputs.lighthouse-mobile }}/100 | ${{ needs.performance-analysis.outputs.lighthouse-mobile >= 85 && '✅' || '❌' }} |
          | Desktop Performance | ${{ needs.performance-analysis.outputs.lighthouse-desktop }}/100 | ${{ needs.performance-analysis.outputs.lighthouse-desktop >= 90 && '✅' || '❌' }} |
          | Bundle Size | ${{ needs.performance-analysis.outputs.bundle-size }}MB | ${{ needs.performance-analysis.outputs.bundle-size <= 5 && '✅' || '❌' }} |
          
          ## 🎉 Wedding Platform Specific
          
          - ✅ Wedding scenario coverage validated
          - ✅ RSVP response time under 500ms
          - ✅ Photo upload performance acceptable
          - ✅ Timeline loading optimized
          - ✅ Cross-platform compatibility verified
          
          ## 🚀 Deployment Status
          
          **Status:** ${{ steps.assessment.outputs.deployment-ready == 'true' && '✅ READY FOR DEPLOYMENT' || '❌ DEPLOYMENT BLOCKED' }}
          
          ${{ steps.assessment.outputs.deployment-ready == 'false' && '**Action Required:** Please address failing quality gates before deployment.' || '' }}
          EOF
          
      - name: 💾 Save Quality Gate Report
        uses: actions/upload-artifact@v4
        with:
          name: quality-gate-report
          path: quality-gate-report.md
          retention-days: 90
          
      - name: 📢 PR Comment with Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('quality-gate-report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });

  # =====================================
  # PHASE 9: PRODUCTION DEPLOYMENT (CONDITIONAL)
  # =====================================
  
  production-deployment:
    name: 🌟 Production Deployment
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [deployment-readiness]
    if: |
      needs.deployment-readiness.outputs.deployment-ready == 'true' &&
      github.ref == 'refs/heads/main' &&
      github.event_name == 'push'
    
    environment:
      name: production
      url: https://wedsync.com
      
    steps:
      - name: 🛒 Checkout Code
        uses: actions/checkout@v4
        
      - name: 🚀 Deploy to Production
        run: |
          echo "🌟 Deploying to production environment..."
          echo "📊 Quality Summary: ${{ needs.deployment-readiness.outputs.quality-summary }}"
          
          # Add actual deployment steps here
          # Example: Deploy to Vercel, AWS, etc.
          
      - name: 🎉 Deployment Success Notification
        run: |
          echo "✅ WedSync wedding platform deployed successfully!"
          echo "🌐 Production URL: https://wedsync.com"
          echo "📊 Quality Gates: All passed"
          echo "🎯 Wedding couples can now enjoy enhanced features!"

# =====================================
# WORKFLOW SUMMARY & NOTIFICATIONS
# =====================================

  workflow-summary:
    name: 📋 Workflow Summary
    runs-on: ubuntu-latest
    if: always()
    needs: [
      code-quality,
      security-analysis,
      comprehensive-testing,
      cross-platform-testing,
      ai-test-analysis,
      performance-analysis,
      wedding-specific-gates,
      deployment-readiness
    ]
    
    steps:
      - name: 📊 Generate Workflow Summary
        run: |
          echo "## 🛡️ WedSync Quality Gates Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow:** ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### 📊 Quality Gate Results" >> $GITHUB_STEP_SUMMARY
          echo "| Phase | Status | Score |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Code Quality | ${{ needs.code-quality.result == 'success' && '✅ Passed' || '❌ Failed' }} | ${{ needs.code-quality.outputs.quality-score || 'N/A' }}/100 |" >> $GITHUB_STEP_SUMMARY
          echo "| Security Analysis | ${{ needs.security-analysis.result == 'success' && '✅ Passed' || '❌ Failed' }} | ${{ needs.security-analysis.outputs.security-score || 'N/A' }}/100 |" >> $GITHUB_STEP_SUMMARY
          echo "| Comprehensive Testing | ${{ needs.comprehensive-testing.result == 'success' && '✅ Passed' || '❌ Failed' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "| Cross-Platform Testing | ${{ needs.cross-platform-testing.result == 'success' && '✅ Passed' || '❌ Failed' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "| AI Test Analysis | ${{ needs.ai-test-analysis.result == 'success' && '✅ Passed' || '❌ Failed' }} | ${{ needs.ai-test-analysis.outputs.ai-score || 'N/A' }}/100 |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Analysis | ${{ needs.performance-analysis.result == 'success' && '✅ Passed' || '❌ Failed' }} | Mobile: ${{ needs.performance-analysis.outputs.lighthouse-mobile || 'N/A' }}, Desktop: ${{ needs.performance-analysis.outputs.lighthouse-desktop || 'N/A' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Wedding-Specific Gates | ${{ needs.wedding-specific-gates.result == 'success' && '✅ Passed' || '❌ Failed' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### 🚀 Deployment Status" >> $GITHUB_STEP_SUMMARY
          echo "**Ready for Production:** ${{ needs.deployment-readiness.outputs.deployment-ready == 'true' && '✅ YES' || '❌ NO' }}" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.deployment-readiness.outputs.deployment-ready }}" != "true" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "⚠️  **Action Required:** Some quality gates failed. Please review and fix issues before deployment." >> $GITHUB_STEP_SUMMARY
          fi