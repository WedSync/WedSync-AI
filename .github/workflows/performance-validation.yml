name: Performance Validation

on:
  # Trigger on pull requests to main branch
  pull_request:
    branches: [main, master, develop]
    paths:
      - 'src/**'
      - 'public/**'
      - 'next.config.ts'
      - 'package.json'
      - 'package-lock.json'
      - '.github/workflows/performance-validation.yml'
  
  # Trigger on pushes to main branch (for production deployments)
  push:
    branches: [main, master]
    paths:
      - 'src/**'
      - 'public/**'
      - 'next.config.ts'
      - 'package.json'
      - 'package-lock.json'
  
  # Manual trigger with custom parameters
  workflow_dispatch:
    inputs:
      testType:
        description: 'Type of performance test'
        required: true
        default: 'load'
        type: choice
        options:
          - load
          - stress
          - spike
          - volume
      
      environment:
        description: 'Target environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      
      testDuration:
        description: 'Test duration in seconds'
        required: false
        default: '300'
        type: string
      
      userCount:
        description: 'Number of virtual users'
        required: false
        default: '100'
        type: string
      
      buildId:
        description: 'Build ID (auto-generated if empty)'
        required: false
        type: string
      
      gitHash:
        description: 'Git commit hash (auto-detected if empty)'
        required: false
        type: string

  # Trigger via API call from other systems
  repository_dispatch:
    types: [performance-validation-requested]

# Environment variables for the workflow
env:
  NODE_VERSION: '20'
  PERFORMANCE_API_URL: ${{ secrets.PERFORMANCE_API_URL || 'https://wedsync.com/api/ci-cd/performance' }}
  LIGHTHOUSE_CI_URL: ${{ secrets.LIGHTHOUSE_CI_URL || 'https://wedsync.com' }}
  K6_CLOUD_TOKEN: ${{ secrets.K6_CLOUD_TOKEN }}
  INTERNAL_API_TOKEN: ${{ secrets.INTERNAL_API_TOKEN }}

# Concurrency control - cancel previous runs on new commits
concurrency:
  group: performance-validation-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # Job 1: Setup and Validation
  setup:
    name: Setup Performance Testing
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    outputs:
      build-id: ${{ steps.setup.outputs.build-id }}
      test-config: ${{ steps.setup.outputs.test-config }}
      should-run-tests: ${{ steps.setup.outputs.should-run-tests }}
      environment: ${{ steps.setup.outputs.environment }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 2 # Need previous commit for comparison
      
      - name: Setup job parameters
        id: setup
        run: |
          # Generate build ID
          BUILD_ID="${{ inputs.buildId || github.run_id }}_${{ github.sha }}"
          echo "build-id=${BUILD_ID}" >> $GITHUB_OUTPUT
          
          # Determine environment
          if [[ "${{ github.ref }}" == "refs/heads/main" || "${{ github.ref }}" == "refs/heads/master" ]]; then
            ENVIRONMENT="production"
          else
            ENVIRONMENT="${{ inputs.environment || 'staging' }}"
          fi
          echo "environment=${ENVIRONMENT}" >> $GITHUB_OUTPUT
          
          # Check if we should run performance tests
          SHOULD_RUN="true"
          
          # Skip if it's a documentation-only change
          if git diff --name-only HEAD~1 | grep -E '\.(md|txt)$' | wc -l | grep -q "^$(git diff --name-only HEAD~1 | wc -l)$"; then
            echo "üìù Documentation-only change detected, skipping performance tests"
            SHOULD_RUN="false"
          fi
          
          echo "should-run-tests=${SHOULD_RUN}" >> $GITHUB_OUTPUT
          
          # Create test configuration
          TEST_CONFIG=$(cat <<EOF
          {
            "testType": "${{ inputs.testType || 'load' }}",
            "duration": ${{ inputs.testDuration || '300' }},
            "userCount": ${{ inputs.userCount || '100' }},
            "targetUrl": "https://wedsync${ENVIRONMENT == 'staging' && '-staging' || ''}.com",
            "environment": "${ENVIRONMENT}",
            "thresholds": {
              "responseTime": ${ENVIRONMENT == 'production' && '2000' || '3000'},
              "errorRate": ${ENVIRONMENT == 'production' && '0.01' || '0.02'},
              "throughput": ${ENVIRONMENT == 'production' && '100' || '50'},
              "coreWebVitals": {
                "LCP": ${ENVIRONMENT == 'production' && '2500' || '3500'},
                "FID": ${ENVIRONMENT == 'production' && '100' || '150'},
                "CLS": ${ENVIRONMENT == 'production' && '0.1' || '0.15'},
                "TTFB": ${ENVIRONMENT == 'production' && '800' || '1200'}
              }
            },
            "weddingContext": {
              "peakSeason": $(date +%m | awk '{if($1>=5 && $1<=10) print "true"; else print "false"}'),
              "criticalPeriod": $(date +%u | awk '{if($1>=5) print "true"; else print "false"}')
            }
          }
          EOF
          )
          
          echo "test-config=${TEST_CONFIG}" >> $GITHUB_OUTPUT
          
          echo "‚úÖ Setup completed:"
          echo "  Build ID: ${BUILD_ID}"
          echo "  Environment: ${ENVIRONMENT}"
          echo "  Should run tests: ${SHOULD_RUN}"

  # Job 2: Build Application
  build:
    name: Build Application
    runs-on: ubuntu-latest
    needs: setup
    if: needs.setup.outputs.should-run-tests == 'true'
    timeout-minutes: 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: 'package-lock.json'
      
      - name: Install dependencies
        run: |
          echo "üì¶ Installing dependencies..."
          npm ci --prefer-offline --no-audit
      
      - name: Build application
        run: |
          echo "üèóÔ∏è Building application for performance testing..."
          npm run build
        env:
          NODE_ENV: production
          NEXT_TELEMETRY_DISABLED: 1
      
      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-artifacts
          path: |
            .next/
            public/
          retention-days: 1

  # Job 3: Lighthouse Performance Audit
  lighthouse:
    name: Lighthouse Performance Audit
    runs-on: ubuntu-latest
    needs: [setup, build]
    if: needs.setup.outputs.should-run-tests == 'true'
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Download build artifacts
        uses: actions/download-artifact@v4
        with:
          name: build-artifacts
      
      - name: Install Lighthouse CI
        run: |
          npm install -g @lhci/cli@latest lighthouse@latest
      
      - name: Start application server
        run: |
          npm install -g serve
          serve -s .next/static -p 3000 &
          echo "SERVER_PID=$!" >> $GITHUB_ENV
          
          # Wait for server to be ready
          timeout 60 bash -c 'until curl -s http://localhost:3000/health > /dev/null; do sleep 1; done' || {
            echo "‚ö†Ô∏è Server failed to start, using production URL"
            echo "USE_PRODUCTION_URL=true" >> $GITHUB_ENV
          }
      
      - name: Run Lighthouse CI
        run: |
          if [[ "$USE_PRODUCTION_URL" == "true" ]]; then
            TARGET_URL="${{ env.LIGHTHOUSE_CI_URL }}"
          else
            TARGET_URL="http://localhost:3000"
          fi
          
          echo "üîç Running Lighthouse audit on: $TARGET_URL"
          
          lhci autorun \
            --collect.url="$TARGET_URL" \
            --collect.url="$TARGET_URL/dashboard" \
            --collect.url="$TARGET_URL/planning" \
            --collect.numberOfRuns=3 \
            --collect.settings.chromeFlags="--no-sandbox --disable-gpu --headless" \
            --assert.assertions.categories:performance=warn \
            --assert.assertions.categories:accessibility=error \
            --assert.assertions.first-contentful-paint=warn \
            --assert.assertions.largest-contentful-paint=error \
            --assert.assertions.cumulative-layout-shift=error \
            --upload.target=temporary-public-storage
        env:
          LHCI_GITHUB_APP_TOKEN: ${{ secrets.LHCI_GITHUB_APP_TOKEN }}
      
      - name: Upload Lighthouse results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: lighthouse-results
          path: |
            .lighthouseci/
            *.json
          retention-days: 7
      
      - name: Cleanup server
        if: always()
        run: |
          if [[ -n "$SERVER_PID" ]]; then
            kill $SERVER_PID || true
          fi

  # Job 4: K6 Load Testing
  k6-load-test:
    name: K6 Load Testing
    runs-on: ubuntu-latest
    needs: [setup, build]
    if: needs.setup.outputs.should-run-tests == 'true'
    timeout-minutes: 20
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup K6
        uses: grafana/setup-k6-action@v1
        with:
          token: ${{ env.K6_CLOUD_TOKEN }}
      
      - name: Create K6 test script
        run: |
          cat > performance-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Rate } from 'k6/metrics';
          
          const errorRate = new Rate('errors');
          const testConfig = JSON.parse(__ENV.TEST_CONFIG);
          
          export const options = {
            stages: [
              { duration: '2m', target: Math.floor(testConfig.userCount * 0.5) },
              { duration: `${Math.floor(testConfig.duration * 0.6)}s`, target: testConfig.userCount },
              { duration: `${Math.floor(testConfig.duration * 0.3)}s`, target: Math.floor(testConfig.userCount * 0.3) },
              { duration: '1m', target: 0 },
            ],
            thresholds: {
              http_req_duration: [`p(95)<${testConfig.thresholds.responseTime}`],
              http_req_failed: [`rate<${testConfig.thresholds.errorRate}`],
              errors: [`rate<${testConfig.thresholds.errorRate}`],
            },
          };
          
          const BASE_URL = testConfig.targetUrl;
          
          // Wedding-specific user scenarios
          const scenarios = [
            { name: 'couple_planning', weight: 40, paths: ['/', '/dashboard', '/planning', '/guests'] },
            { name: 'photographer_portfolio', weight: 30, paths: ['/', '/gallery', '/portfolio', '/bookings'] },
            { name: 'vendor_management', weight: 20, paths: ['/', '/vendor-dashboard', '/calendar', '/messages'] },
            { name: 'guest_interaction', weight: 10, paths: ['/', '/rsvp', '/photos', '/timeline'] }
          ];
          
          export default function () {
            // Select scenario based on weight
            const scenario = scenarios[Math.floor(Math.random() * scenarios.length)];
            const paths = scenario.paths;
            
            // Execute scenario
            for (const path of paths) {
              const response = http.get(`${BASE_URL}${path}`, {
                tags: { scenario: scenario.name, path: path },
                timeout: '30s',
              });
              
              const success = check(response, {
                'status is 200': (r) => r.status === 200,
                'response time < threshold': (r) => r.timings.duration < testConfig.thresholds.responseTime,
                'no server errors': (r) => r.status < 500,
              });
              
              errorRate.add(!success);
              
              if (!success) {
                console.error(`‚ùå Failed check for ${path}: ${response.status}`);
              }
              
              // Wedding platform realistic user behavior
              sleep(Math.random() * 2 + 1); // 1-3 seconds between requests
            }
          }
          EOF
          
          echo "üìù K6 test script created"
      
      - name: Run K6 load test
        run: |
          echo "‚ö° Starting K6 load test..."
          
          TEST_CONFIG='${{ needs.setup.outputs.test-config }}'
          export TEST_CONFIG
          
          k6 run \
            --out json=k6-results.json \
            --summary-trend-stats="avg,min,med,max,p(90),p(95)" \
            performance-test.js
        continue-on-error: true # Don't fail workflow if load test finds issues
      
      - name: Upload K6 results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: k6-results
          path: k6-results.json
          retention-days: 7

  # Job 5: Performance Analysis and Validation
  analyze-results:
    name: Analyze Performance Results
    runs-on: ubuntu-latest
    needs: [setup, lighthouse, k6-load-test]
    if: always() && needs.setup.outputs.should-run-tests == 'true'
    timeout-minutes: 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
      
      - name: Analyze performance results
        id: analysis
        run: |
          echo "üìä Analyzing performance results..."
          
          # Initialize results
          VALIDATION_PASSED="true"
          VIOLATIONS=()
          
          # Analyze Lighthouse results
          if [[ -f "artifacts/lighthouse-results/.lighthouseci/manifest.json" ]]; then
            echo "üîç Processing Lighthouse results..."
            
            # Extract Core Web Vitals
            LCP=$(cat artifacts/lighthouse-results/.lighthouseci/manifest.json | jq -r '.[0].summary.performance // 0')
            
            # Mock analysis - in production, parse actual Lighthouse JSON
            if (( $(echo "$LCP < 80" | bc -l) )); then
              VALIDATION_PASSED="false"
              VIOLATIONS+=("Lighthouse performance score below 80")
            fi
          fi
          
          # Analyze K6 results
          if [[ -f "artifacts/k6-results/k6-results.json" ]]; then
            echo "‚ö° Processing K6 results..."
            
            # Extract key metrics
            AVG_RESPONSE_TIME=$(tail -1 artifacts/k6-results/k6-results.json | jq -r '.metrics.http_req_duration.values.avg // 0')
            ERROR_RATE=$(tail -1 artifacts/k6-results/k6-results.json | jq -r '.metrics.http_req_failed.values.rate // 0')
            
            echo "üìà K6 Results:"
            echo "  Average response time: ${AVG_RESPONSE_TIME}ms"
            echo "  Error rate: ${ERROR_RATE}"
            
            # Check thresholds
            TEST_CONFIG='${{ needs.setup.outputs.test-config }}'
            THRESHOLD_RESPONSE=$(echo "$TEST_CONFIG" | jq -r '.thresholds.responseTime')
            THRESHOLD_ERROR=$(echo "$TEST_CONFIG" | jq -r '.thresholds.errorRate')
            
            if (( $(echo "$AVG_RESPONSE_TIME > $THRESHOLD_RESPONSE" | bc -l) )); then
              VALIDATION_PASSED="false"
              VIOLATIONS+=("Average response time ${AVG_RESPONSE_TIME}ms exceeds threshold ${THRESHOLD_RESPONSE}ms")
            fi
            
            if (( $(echo "$ERROR_RATE > $THRESHOLD_ERROR" | bc -l) )); then
              VALIDATION_PASSED="false"
              VIOLATIONS+=("Error rate ${ERROR_RATE} exceeds threshold ${THRESHOLD_ERROR}")
            fi
          fi
          
          # Set outputs
          echo "validation-passed=${VALIDATION_PASSED}" >> $GITHUB_OUTPUT
          echo "violations-count=${#VIOLATIONS[@]}" >> $GITHUB_OUTPUT
          
          # Create violations summary
          VIOLATIONS_JSON=$(printf '%s\n' "${VIOLATIONS[@]}" | jq -R . | jq -s .)
          echo "violations=${VIOLATIONS_JSON}" >> $GITHUB_OUTPUT
          
          echo "üìã Performance Analysis Complete:"
          echo "  Validation passed: ${VALIDATION_PASSED}"
          echo "  Violations count: ${#VIOLATIONS[@]}"
          
          if [[ "${#VIOLATIONS[@]}" -gt 0 ]]; then
            echo "  Violations:"
            printf '    - %s\n' "${VIOLATIONS[@]}"
          fi

  # Job 6: Update Performance Validation Status
  update-status:
    name: Update Performance Status
    runs-on: ubuntu-latest
    needs: [setup, analyze-results]
    if: always() && needs.setup.outputs.should-run-tests == 'true'
    timeout-minutes: 5
    
    steps:
      - name: Report validation results
        run: |
          BUILD_ID="${{ needs.setup.outputs.build-id }}"
          VALIDATION_PASSED="${{ needs.analyze-results.outputs.validation-passed }}"
          VIOLATIONS='${{ needs.analyze-results.outputs.violations }}'
          ENVIRONMENT="${{ needs.setup.outputs.environment }}"
          
          echo "üì° Reporting performance validation results..."
          echo "  Build ID: ${BUILD_ID}"
          echo "  Validation passed: ${VALIDATION_PASSED}"
          echo "  Environment: ${ENVIRONMENT}"
          
          # Call performance validation API
          VALIDATION_PAYLOAD=$(cat <<EOF
          {
            "buildId": "${BUILD_ID}",
            "environment": "${ENVIRONMENT}",
            "gitHash": "${{ github.sha }}",
            "testConfig": ${{ needs.setup.outputs.test-config }},
            "deploymentContext": {
              "buildId": "${BUILD_ID}",
              "gitHash": "${{ github.sha }}",
              "branch": "${{ github.ref_name }}",
              "environment": "${ENVIRONMENT}",
              "triggeredBy": "${{ github.actor }}",
              "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
            },
            "triggerSource": "github_actions"
          }
          EOF
          )
          
          # Make API call (with retries)
          for i in {1..3}; do
            if curl -X POST "${{ env.PERFORMANCE_API_URL }}/validate" \
              -H "Content-Type: application/json" \
              -H "Authorization: Bearer ${{ secrets.INTERNAL_API_TOKEN }}" \
              -d "$VALIDATION_PAYLOAD" \
              --max-time 60 \
              --retry 2; then
              echo "‚úÖ Performance validation results reported successfully"
              break
            else
              echo "‚ö†Ô∏è Failed to report results, attempt $i/3"
              if [[ $i -eq 3 ]]; then
                echo "‚ùå Failed to report performance validation results after 3 attempts"
              else
                sleep 10
              fi
            fi
          done
      
      - name: Set final status
        if: always()
        run: |
          if [[ "${{ needs.analyze-results.outputs.validation-passed }}" == "true" ]]; then
            echo "‚úÖ Performance validation PASSED"
            exit 0
          else
            echo "‚ùå Performance validation FAILED"
            echo "Violations:"
            echo '${{ needs.analyze-results.outputs.violations }}' | jq -r '.[]'
            exit 1
          fi

  # Job 7: Wedding Platform Context Summary
  wedding-context-summary:
    name: Wedding Platform Performance Summary
    runs-on: ubuntu-latest
    needs: [setup, analyze-results]
    if: always() && needs.setup.outputs.should-run-tests == 'true'
    timeout-minutes: 5
    
    steps:
      - name: Generate wedding context summary
        run: |
          echo "üíí WedSync Performance Validation Summary"
          echo "========================================"
          echo ""
          echo "**Build Information:**"
          echo "- Build ID: ${{ needs.setup.outputs.build-id }}"
          echo "- Environment: ${{ needs.setup.outputs.environment }}"
          echo "- Branch: ${{ github.ref_name }}"
          echo "- Commit: ${{ github.sha }}"
          echo "- Triggered by: ${{ github.actor }}"
          echo ""
          echo "**Wedding Platform Context:**"
          echo "- Peak Season: $(date +%m | awk '{if($1>=5 && $1<=10) print "Yes (May-Oct)"; else print "No"}')"
          echo "- Critical Period: $(date +%u | awk '{if($1>=5) print "Yes (Weekend)"; else print "No (Weekday)"}')"
          echo "- Test Time: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          echo ""
          echo "**Performance Impact Areas:**"
          echo "- üë∞ Wedding Couples: Timeline planning, guest management, vendor communication"
          echo "- üì∏ Photographers: Portfolio display, photo uploads, booking management"
          echo "- üè™ Vendors: Service listings, calendar updates, client communication"
          echo "- üë• Guests: RSVP functionality, photo sharing, event details"
          echo ""
          echo "**Validation Result:**"
          if [[ "${{ needs.analyze-results.outputs.validation-passed }}" == "true" ]]; then
            echo "‚úÖ PASSED - Performance standards met for wedding platform users"
          else
            echo "‚ùå FAILED - Performance issues detected that could impact wedding experience"
            echo ""
            echo "**Critical Issues:**"
            echo '${{ needs.analyze-results.outputs.violations }}' | jq -r '.[] | "- " + .'
            echo ""
            echo "**Recommended Actions:**"
            echo "1. Review and optimize slow database queries affecting wedding data"
            echo "2. Implement image optimization for wedding photo galleries"
            echo "3. Add caching for frequently accessed wedding vendor data"
            echo "4. Test performance during peak wedding planning hours (evenings/weekends)"
          fi
          echo ""
          echo "**Next Steps:**"
          echo "1. Monitor real user metrics from wedding couples and vendors"
          echo "2. Schedule regular performance reviews during peak wedding season"
          echo "3. Implement continuous monitoring for critical wedding workflows"
          echo "4. Maintain performance budgets aligned with wedding industry expectations"