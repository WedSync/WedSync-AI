import { RateLimiter } from '@/lib/rate-limit';
import { performance } from 'perf_hooks';

describe('WS-199 Rate Limiting Performance Benchmarks', () => {
  let rateLimiter: RateLimiter;

  beforeEach(() => {
    rateLimiter = new RateLimiter({
      windowMs: 60000, // 1 minute
      maxRequests: 100,
    });
  });

  describe('Core Performance Requirements (<5ms)', () => {
    it('should complete single rate limit checks within 5ms', async () => {
      console.log('‚ö° Testing single request performance...');
      
      const performanceResults: Array<{
        iteration: number;
        responseTime: number;
        success: boolean;
        meetsRequirement: boolean;
      }> = [];

      // Run 1000 individual rate limit checks
      for (let i = 0; i < 1000; i++) {
        const testKey = `performance-test-single-${i}`;
        
        const startTime = performance.now();
        const result = await rateLimiter.checkLimit(testKey);
        const endTime = performance.now();
        
        const responseTime = endTime - startTime;
        const meetsRequirement = responseTime < 5;
        
        performanceResults.push({
          iteration: i + 1,
          responseTime,
          success: result.success,
          meetsRequirement
        });
        
        // Log progress every 200 iterations
        if ((i + 1) % 200 === 0) {
          const recentAvg = performanceResults
            .slice(-200)
            .reduce((sum, r) => sum + r.responseTime, 0) / 200;
          console.log(`üìä Iteration ${i + 1}: Recent 200 avg = ${recentAvg.toFixed(3)}ms`);
        }
      }

      // Calculate performance statistics
      const responseTimes = performanceResults.map(r => r.responseTime);
      const averageResponseTime = responseTimes.reduce((sum, time) => sum + time, 0) / responseTimes.length;
      const minResponseTime = Math.min(...responseTimes);
      const maxResponseTime = Math.max(...responseTimes);
      
      // Calculate percentiles
      const sortedTimes = responseTimes.sort((a, b) => a - b);
      const p50 = sortedTimes[Math.floor(sortedTimes.length * 0.50)];
      const p90 = sortedTimes[Math.floor(sortedTimes.length * 0.90)];
      const p95 = sortedTimes[Math.floor(sortedTimes.length * 0.95)];
      const p99 = sortedTimes[Math.floor(sortedTimes.length * 0.99)];
      
      // Count requests meeting performance requirement
      const meetingRequirement = performanceResults.filter(r => r.meetsRequirement).length;
      const performanceComplianceRate = (meetingRequirement / performanceResults.length) * 100;
      
      // Performance requirement validation
      expect(averageResponseTime).toBeLessThan(5); // <5ms average requirement
      expect(p95).toBeLessThan(10); // <10ms P95 requirement
      expect(p99).toBeLessThan(25); // <25ms P99 requirement
      expect(performanceComplianceRate).toBeGreaterThan(95); // >95% requests meet requirement

      console.log('‚úÖ Single Request Performance Results:', {
        totalRequests: performanceResults.length,
        averageResponseTime: `${averageResponseTime.toFixed(3)}ms`,
        minResponseTime: `${minResponseTime.toFixed(3)}ms`,
        maxResponseTime: `${maxResponseTime.toFixed(3)}ms`,
        p50: `${p50.toFixed(3)}ms`,
        p90: `${p90.toFixed(3)}ms`,
        p95: `${p95.toFixed(3)}ms`,
        p99: `${p99.toFixed(3)}ms`,
        performanceComplianceRate: `${performanceComplianceRate.toFixed(2)}%`
      });
    });

    it('should maintain performance under concurrent access', async () => {
      console.log('üîÑ Testing concurrent request performance...');
      
      const concurrentUsers = 100;
      const requestsPerUser = 10;
      const totalRequests = concurrentUsers * requestsPerUser;
      
      const concurrentResults: Array<{
        userId: number;
        requestId: number;
        responseTime: number;
        success: boolean;
        timestamp: number;
      }> = [];

      const startTime = performance.now();

      // Create concurrent user simulations
      const userPromises = Array.from({ length: concurrentUsers }, async (_, userId) => {
        const userResults: Array<{
          userId: number;
          requestId: number;
          responseTime: number;
          success: boolean;
          timestamp: number;
        }> = [];

        // Each user makes multiple requests
        for (let requestId = 0; requestId < requestsPerUser; requestId++) {
          const userKey = `concurrent-user-${userId}`;
          
          const requestStart = performance.now();
          const result = await rateLimiter.checkLimit(userKey);
          const requestEnd = performance.now();
          
          userResults.push({
            userId,
            requestId,
            responseTime: requestEnd - requestStart,
            success: result.success,
            timestamp: requestEnd
          });
        }

        return userResults;
      });

      // Execute all concurrent users
      const allUserResults = await Promise.all(userPromises);
      concurrentResults.push(...allUserResults.flat());

      const totalDuration = performance.now() - startTime;

      // Analyze concurrent performance
      const concurrentResponseTimes = concurrentResults.map(r => r.responseTime);
      const avgConcurrentResponseTime = concurrentResponseTimes.reduce((sum, time) => sum + time, 0) / concurrentResponseTimes.length;
      const concurrentP95 = concurrentResponseTimes.sort((a, b) => a - b)[Math.floor(concurrentResponseTimes.length * 0.95)];
      const concurrentThroughput = totalRequests / (totalDuration / 1000); // requests per second

      // Concurrent performance validation
      expect(avgConcurrentResponseTime).toBeLessThan(5); // <5ms even under concurrency
      expect(concurrentP95).toBeLessThan(15); // <15ms P95 under concurrency
      expect(concurrentThroughput).toBeGreaterThan(1000); // >1000 RPS throughput
      expect(totalDuration).toBeLessThan(5000); // Complete within 5 seconds

      console.log('‚úÖ Concurrent Performance Results:', {
        concurrentUsers,
        totalRequests,
        totalDuration: `${totalDuration.toFixed(2)}ms`,
        avgConcurrentResponseTime: `${avgConcurrentResponseTime.toFixed(3)}ms`,
        concurrentP95: `${concurrentP95.toFixed(3)}ms`,
        throughput: `${concurrentThroughput.toFixed(2)} RPS`
      });
    });

    it('should perform efficiently with high memory pressure', async () => {
      console.log('üíæ Testing performance under high memory usage...');
      
      const highMemoryLimiter = new RateLimiter({
        windowMs: 60000,
        maxRequests: 1000, // High limit to avoid blocking
      });

      // Create many unique keys to increase memory usage
      const uniqueKeys = 10000;
      const memoryPressureResults: Array<{
        keyCount: number;
        responseTime: number;
        memoryUsage: number;
      }> = [];

      for (let keyCount = 1; keyCount <= uniqueKeys; keyCount++) {
        const uniqueKey = `memory-pressure-key-${keyCount}`;
        
        const startTime = performance.now();
        await highMemoryLimiter.checkLimit(uniqueKey);
        const endTime = performance.now();
        
        const responseTime = endTime - startTime;
        
        // Estimate memory usage (in real implementation, would use process.memoryUsage())
        const estimatedMemoryUsage = keyCount * 100; // Rough estimate in bytes
        
        memoryPressureResults.push({
          keyCount,
          responseTime,
          memoryUsage: estimatedMemoryUsage
        });

        // Log progress every 2000 keys
        if (keyCount % 2000 === 0) {
          const recentAvgTime = memoryPressureResults
            .slice(-100)
            .reduce((sum, r) => sum + r.responseTime, 0) / 100;
          console.log(`üìà ${keyCount} keys: Recent avg time = ${recentAvgTime.toFixed(3)}ms`);
        }
      }

      // Analyze memory pressure performance
      const finalResults = memoryPressureResults.slice(-1000); // Last 1000 requests
      const avgTimeUnderPressure = finalResults.reduce((sum, r) => sum + r.responseTime, 0) / finalResults.length;
      const maxTimeUnderPressure = Math.max(...finalResults.map(r => r.responseTime));
      
      // Memory pressure performance validation
      expect(avgTimeUnderPressure).toBeLessThan(5); // <5ms even with high memory usage
      expect(maxTimeUnderPressure).toBeLessThan(25); // <25ms maximum even under pressure
      
      // Memory growth should be reasonable
      const finalMemoryUsage = memoryPressureResults[memoryPressureResults.length - 1].memoryUsage;
      expect(finalMemoryUsage).toBeLessThan(2 * 1024 * 1024); // <2MB estimated usage
      
      console.log('‚úÖ High Memory Pressure Results:', {
        uniqueKeys,
        avgTimeUnderPressure: `${avgTimeUnderPressure.toFixed(3)}ms`,
        maxTimeUnderPressure: `${maxTimeUnderPressure.toFixed(3)}ms`,
        estimatedMemoryUsage: `${(finalMemoryUsage / 1024).toFixed(2)}KB`
      });
    });
  });

  describe('Wedding Industry Specific Performance', () => {
    it('should handle wedding season traffic burst performance', async () => {
      console.log('üå∏ Testing wedding season traffic burst performance...');
      
      // Simulate wedding season multiplier (2x normal traffic)
      const seasonalLimiter = new RateLimiter({
        windowMs: 60000,
        maxRequests: 200, // 2x normal limit for wedding season
      });

      const burstResults: Array<{
        burstId: number;
        burstSize: number;
        burstResponseTime: number;
        burstThroughput: number;
        allRequestsSucceeded: boolean;
      }> = [];

      // Simulate 10 different wedding traffic bursts
      for (let burstId = 1; burstId <= 10; burstId++) {
        const burstSize = 50 + (burstId * 10); // Increasing burst size
        const burstKey = `wedding-burst-${burstId}`;
        
        const burstStart = performance.now();
        const burstPromises = Array.from({ length: burstSize }, async (_, requestId) => {
          const requestStart = performance.now();
          const result = await seasonalLimiter.checkLimit(`${burstKey}-${requestId}`);
          const requestEnd = performance.now();
          
          return {
            success: result.success,
            responseTime: requestEnd - requestStart
          };
        });

        const burstRequestResults = await Promise.all(burstPromises);
        const burstEnd = performance.now();

        const burstResponseTime = burstEnd - burstStart;
        const burstThroughput = burstSize / (burstResponseTime / 1000);
        const allRequestsSucceeded = burstRequestResults.every(r => r.success);
        
        burstResults.push({
          burstId,
          burstSize,
          burstResponseTime,
          burstThroughput,
          allRequestsSucceeded
        });

        console.log(`üí• Burst ${burstId}: ${burstSize} requests in ${burstResponseTime.toFixed(2)}ms`);

        // Brief pause between bursts
        await new Promise(resolve => setTimeout(resolve, 100));
      }

      // Analyze burst performance
      const avgBurstResponseTime = burstResults.reduce((sum, r) => sum + r.burstResponseTime, 0) / burstResults.length;
      const avgBurstThroughput = burstResults.reduce((sum, r) => sum + r.burstThroughput, 0) / burstResults.length;
      const successfulBursts = burstResults.filter(r => r.allRequestsSucceeded).length;
      
      // Wedding season burst validation
      expect(avgBurstResponseTime).toBeLessThan(500); // <500ms for burst completion
      expect(avgBurstThroughput).toBeGreaterThan(500); // >500 RPS burst throughput
      expect(successfulBursts).toBeGreaterThan(5); // Most bursts should succeed
      
      console.log('‚úÖ Wedding Season Burst Results:', {
        totalBursts: burstResults.length,
        avgBurstResponseTime: `${avgBurstResponseTime.toFixed(2)}ms`,
        avgBurstThroughput: `${avgBurstThroughput.toFixed(2)} RPS`,
        successfulBursts: `${successfulBursts}/${burstResults.length}`
      });
    });

    it('should maintain performance during Saturday wedding rush', async () => {
      console.log('üíí Testing Saturday wedding rush performance...');
      
      // Mock Saturday evening (peak wedding time)
      const originalDate = Date;
      global.Date = class extends Date {
        constructor() {
          super('2025-06-14T18:00:00'); // Saturday 6 PM in June
        }
        static now() {
          return new originalDate('2025-06-14T18:00:00').getTime();
        }
        getDay() {
          return 6; // Saturday
        }
        getHours() {
          return 18; // 6 PM
        }
      } as any;

      const saturdayRushLimiter = new RateLimiter({
        windowMs: 60000,
        maxRequests: 500, // High limit for Saturday rush
      });

      const rushResults: Array<{
        vendor: string;
        action: string;
        responseTime: number;
        success: boolean;
        timestamp: number;
      }> = [];

      // Simulate different wedding vendors during Saturday rush
      const vendors = ['photographer', 'venue', 'caterer', 'florist', 'dj'];
      const actions = ['client_update', 'timeline_sync', 'photo_upload', 'status_change'];

      const saturdayRushStart = performance.now();

      // Create Saturday rush scenario
      const vendorPromises = vendors.map(async (vendor) => {
        const vendorResults: typeof rushResults = [];

        for (let i = 0; i < 100; i++) { // Each vendor makes 100 requests
          const action = actions[i % actions.length];
          const rushKey = `saturday-rush-${vendor}-${action}-${i}`;
          
          const requestStart = performance.now();
          const result = await saturdayRushLimiter.checkLimit(rushKey);
          const requestEnd = performance.now();
          
          vendorResults.push({
            vendor,
            action,
            responseTime: requestEnd - requestStart,
            success: result.success,
            timestamp: requestEnd
          });
        }

        return vendorResults;
      });

      const allVendorResults = await Promise.all(vendorPromises);
      rushResults.push(...allVendorResults.flat());

      const saturdayRushDuration = performance.now() - saturdayRushStart;

      // Restore original Date
      global.Date = originalDate;

      // Analyze Saturday rush performance
      const rushResponseTimes = rushResults.map(r => r.responseTime);
      const avgSaturdayResponseTime = rushResponseTimes.reduce((sum, time) => sum + time, 0) / rushResponseTimes.length;
      const saturdayP95 = rushResponseTimes.sort((a, b) => a - b)[Math.floor(rushResponseTimes.length * 0.95)];
      const saturdayThroughput = rushResults.length / (saturdayRushDuration / 1000);
      const criticalActionsSucceeded = rushResults.filter(r => 
        ['timeline_sync', 'status_change'].includes(r.action) && r.success
      ).length;

      // Saturday rush performance validation
      expect(avgSaturdayResponseTime).toBeLessThan(5); // <5ms even during Saturday rush
      expect(saturdayP95).toBeLessThan(20); // <20ms P95 during rush
      expect(saturdayThroughput).toBeGreaterThan(800); // >800 RPS during rush
      expect(criticalActionsSucceeded).toBeGreaterThan(rushResults.length * 0.3); // >30% critical actions

      console.log('‚úÖ Saturday Wedding Rush Results:', {
        vendors: vendors.length,
        totalRushRequests: rushResults.length,
        rushDuration: `${saturdayRushDuration.toFixed(2)}ms`,
        avgSaturdayResponseTime: `${avgSaturdayResponseTime.toFixed(3)}ms`,
        saturdayP95: `${saturdayP95.toFixed(3)}ms`,
        saturdayThroughput: `${saturdayThroughput.toFixed(2)} RPS`,
        criticalActionsSucceeded
      });
    });

    it('should perform efficiently during portfolio upload peaks', async () => {
      console.log('üì∏ Testing portfolio upload peak performance...');
      
      const portfolioUploadLimiter = new RateLimiter({
        windowMs: 60000,
        maxRequests: 50, // Photographers upload many images
      });

      const uploadResults: Array<{
        photographerId: number;
        imageId: number;
        uploadSize: number; // bytes
        responseTime: number;
        success: boolean;
        processingTime: number;
      }> = [];

      // Simulate 20 photographers uploading portfolios simultaneously
      const photographerCount = 20;
      const imagesPerPhotographer = 25; // 25 images each

      const portfolioUploadStart = performance.now();

      const photographerPromises = Array.from({ length: photographerCount }, async (_, photographerId) => {
        const photographerResults: typeof uploadResults = [];

        for (let imageId = 1; imageId <= imagesPerPhotographer; imageId++) {
          const uploadKey = `portfolio-upload-photographer-${photographerId}`;
          const uploadSize = 2 * 1024 * 1024 + Math.random() * 3 * 1024 * 1024; // 2-5MB images
          
          const uploadStart = performance.now();
          const result = await portfolioUploadLimiter.checkLimit(uploadKey);
          const uploadEnd = performance.now();
          
          // Simulate image processing time based on size
          const processingTime = uploadSize / (10 * 1024 * 1024) * 100; // ~10ms per MB
          
          photographerResults.push({
            photographerId,
            imageId,
            uploadSize,
            responseTime: uploadEnd - uploadStart,
            success: result.success,
            processingTime
          });
          
          // Small delay between uploads for realism
          await new Promise(resolve => setTimeout(resolve, 50));
        }

        return photographerResults;
      });

      const allPhotographerResults = await Promise.all(photographerPromises);
      uploadResults.push(...allPhotographerResults.flat());

      const portfolioUploadDuration = performance.now() - portfolioUploadStart;

      // Analyze portfolio upload performance
      const uploadResponseTimes = uploadResults.map(r => r.responseTime);
      const avgUploadResponseTime = uploadResponseTimes.reduce((sum, time) => sum + time, 0) / uploadResponseTimes.length;
      const uploadP95 = uploadResponseTimes.sort((a, b) => a - b)[Math.floor(uploadResponseTimes.length * 0.95)];
      const successfulUploads = uploadResults.filter(r => r.success).length;
      const totalDataProcessed = uploadResults.reduce((sum, r) => sum + r.uploadSize, 0);
      const avgProcessingTime = uploadResults.reduce((sum, r) => sum + r.processingTime, 0) / uploadResults.length;

      // Portfolio upload performance validation
      expect(avgUploadResponseTime).toBeLessThan(5); // <5ms rate limit check even for uploads
      expect(uploadP95).toBeLessThan(15); // <15ms P95 for upload rate limiting
      expect(successfulUploads / uploadResults.length).toBeGreaterThan(0.7); // >70% upload success rate
      expect(avgProcessingTime).toBeLessThan(500); // <500ms average processing

      console.log('‚úÖ Portfolio Upload Peak Results:', {
        photographers: photographerCount,
        totalImages: uploadResults.length,
        uploadDuration: `${portfolioUploadDuration.toFixed(2)}ms`,
        avgUploadResponseTime: `${avgUploadResponseTime.toFixed(3)}ms`,
        uploadP95: `${uploadP95.toFixed(3)}ms`,
        successfulUploads: `${successfulUploads}/${uploadResults.length}`,
        totalDataProcessed: `${(totalDataProcessed / (1024 * 1024)).toFixed(2)}MB`,
        avgProcessingTime: `${avgProcessingTime.toFixed(2)}ms`
      });
    });
  });

  describe('System Resource Efficiency', () => {
    it('should maintain CPU efficiency under load', async () => {
      console.log('üñ•Ô∏è  Testing CPU efficiency under load...');
      
      const cpuEfficiencyResults: Array<{
        iteration: number;
        responseTime: number;
        estimatedCpuUsage: number;
        memoryFootprint: number;
      }> = [];

      const iterations = 5000;
      let totalCpuTime = 0;

      for (let iteration = 1; iteration <= iterations; iteration++) {
        const testKey = `cpu-efficiency-${iteration}`;
        
        const cpuStart = process.hrtime.bigint();
        const wallStart = performance.now();
        
        await rateLimiter.checkLimit(testKey);
        
        const wallEnd = performance.now();
        const cpuEnd = process.hrtime.bigint();
        
        const responseTime = wallEnd - wallStart;
        const cpuTime = Number(cpuEnd - cpuStart) / 1e6; // Convert nanoseconds to milliseconds
        totalCpuTime += cpuTime;
        
        // Estimate memory footprint (would use actual memory profiling in production)
        const estimatedMemory = iteration * 50; // Rough estimate in bytes
        
        cpuEfficiencyResults.push({
          iteration,
          responseTime,
          estimatedCpuUsage: cpuTime,
          memoryFootprint: estimatedMemory
        });

        // Log progress every 1000 iterations
        if (iteration % 1000 === 0) {
          const recentCpuAvg = cpuEfficiencyResults
            .slice(-1000)
            .reduce((sum, r) => sum + r.estimatedCpuUsage, 0) / 1000;
          console.log(`‚öôÔ∏è  Iteration ${iteration}: CPU avg = ${recentCpuAvg.toFixed(3)}ms`);
        }
      }

      // Analyze CPU efficiency
      const avgCpuUsage = totalCpuTime / iterations;
      const avgResponseTime = cpuEfficiencyResults.reduce((sum, r) => sum + r.responseTime, 0) / iterations;
      const cpuEfficiency = avgResponseTime / avgCpuUsage; // Lower is better (less CPU per response)
      const finalMemoryFootprint = cpuEfficiencyResults[cpuEfficiencyResults.length - 1].memoryFootprint;
      
      // CPU efficiency validation
      expect(avgCpuUsage).toBeLessThan(1); // <1ms CPU time per request
      expect(avgResponseTime).toBeLessThan(5); // <5ms wall time
      expect(cpuEfficiency).toBeGreaterThan(1); // Efficient CPU usage
      expect(finalMemoryFootprint).toBeLessThan(500 * 1024); // <500KB memory footprint
      
      console.log('‚úÖ CPU Efficiency Results:', {
        iterations,
        avgCpuUsage: `${avgCpuUsage.toFixed(3)}ms`,
        avgResponseTime: `${avgResponseTime.toFixed(3)}ms`,
        cpuEfficiency: cpuEfficiency.toFixed(3),
        finalMemoryFootprint: `${(finalMemoryFootprint / 1024).toFixed(2)}KB`
      });
    });

    it('should perform efficient cleanup operations', async () => {
      console.log('üßπ Testing cleanup operation efficiency...');
      
      const cleanupLimiter = new RateLimiter({
        windowMs: 1000, // 1 second window for quick cleanup testing
        maxRequests: 10,
      });

      const cleanupResults: Array<{
        phase: string;
        beforeCleanupSize: number;
        afterCleanupSize: number;
        cleanupTime: number;
        performanceImpact: number;
      }> = [];

      // Create entries that will expire
      console.log('üìù Creating entries for cleanup testing...');
      for (let i = 0; i < 500; i++) {
        await cleanupLimiter.checkLimit(`cleanup-test-${i}`);
      }

      // Get initial store size (in real implementation, would check actual store)
      const initialStoreSize = 500; // Estimated

      // Wait for entries to expire
      console.log('‚è≥ Waiting for entries to expire...');
      await new Promise(resolve => setTimeout(resolve, 1100)); // 1.1 seconds

      // Test performance impact of cleanup during normal operations
      const phases = ['before-cleanup', 'during-cleanup', 'after-cleanup'];
      
      for (const phase of phases) {
        const phaseStart = performance.now();
        const phaseResults: number[] = [];
        
        // Perform normal operations during each phase
        for (let i = 0; i < 100; i++) {
          const opStart = performance.now();
          await cleanupLimiter.checkLimit(`${phase}-operation-${i}`);
          const opEnd = performance.now();
          
          phaseResults.push(opEnd - opStart);
        }
        
        const phaseEnd = performance.now();
        
        const beforeSize = phase === 'before-cleanup' ? initialStoreSize : 100;
        const afterSize = phase === 'after-cleanup' ? 50 : 100; // Estimated cleanup effect
        const cleanupTime = phaseEnd - phaseStart;
        const avgOperationTime = phaseResults.reduce((sum, time) => sum + time, 0) / phaseResults.length;
        
        cleanupResults.push({
          phase,
          beforeCleanupSize: beforeSize,
          afterCleanupSize: afterSize,
          cleanupTime,
          performanceImpact: avgOperationTime
        });

        console.log(`üìä Phase ${phase}: Avg operation time = ${avgOperationTime.toFixed(3)}ms`);
      }

      // Analyze cleanup efficiency
      const beforeCleanupPerf = cleanupResults.find(r => r.phase === 'before-cleanup')?.performanceImpact || 0;
      const duringCleanupPerf = cleanupResults.find(r => r.phase === 'during-cleanup')?.performanceImpact || 0;
      const afterCleanupPerf = cleanupResults.find(r => r.phase === 'after-cleanup')?.performanceImpact || 0;
      
      const cleanupImpact = duringCleanupPerf - beforeCleanupPerf;
      const performanceImprovement = beforeCleanupPerf - afterCleanupPerf;
      
      // Cleanup efficiency validation
      expect(cleanupImpact).toBeLessThan(2); // <2ms performance impact during cleanup
      expect(afterCleanupPerf).toBeLessThan(5); // <5ms operations after cleanup
      expect(performanceImprovement).toBeGreaterThan(-1); // Should improve or stay same
      
      console.log('‚úÖ Cleanup Efficiency Results:', {
        beforeCleanupPerf: `${beforeCleanupPerf.toFixed(3)}ms`,
        duringCleanupPerf: `${duringCleanupPerf.toFixed(3)}ms`,
        afterCleanupPerf: `${afterCleanupPerf.toFixed(3)}ms`,
        cleanupImpact: `${cleanupImpact.toFixed(3)}ms`,
        performanceImprovement: `${performanceImprovement.toFixed(3)}ms`
      });
    });
  });

  describe('Edge Case Performance', () => {
    it('should handle performance with very long keys', async () => {
      console.log('üîë Testing performance with long rate limit keys...');
      
      const longKeyResults: Array<{
        keyLength: number;
        responseTime: number;
        success: boolean;
      }> = [];

      // Test with increasingly long keys
      const baseLengths = [50, 100, 500, 1000, 2000];
      
      for (const baseLength of baseLengths) {
        const longKey = 'x'.repeat(baseLength);
        
        // Test multiple requests with this key length
        const keyLengthResults: number[] = [];
        
        for (let i = 0; i < 100; i++) {
          const testKey = `${longKey}-${i}`;
          
          const startTime = performance.now();
          const result = await rateLimiter.checkLimit(testKey);
          const endTime = performance.now();
          
          keyLengthResults.push(endTime - startTime);
          longKeyResults.push({
            keyLength: testKey.length,
            responseTime: endTime - startTime,
            success: result.success
          });
        }
        
        const avgTimeForLength = keyLengthResults.reduce((sum, time) => sum + time, 0) / keyLengthResults.length;
        console.log(`üìè Key length ${baseLength}: Avg time = ${avgTimeForLength.toFixed(3)}ms`);
      }

      // Analyze long key performance
      const maxKeyLength = Math.max(...longKeyResults.map(r => r.keyLength));
      const avgTimeForLongKeys = longKeyResults
        .filter(r => r.keyLength > 500)
        .reduce((sum, r) => sum + r.responseTime, 0) / longKeyResults.filter(r => r.keyLength > 500).length;
      
      const avgTimeForShortKeys = longKeyResults
        .filter(r => r.keyLength <= 100)
        .reduce((sum, r) => sum + r.responseTime, 0) / longKeyResults.filter(r => r.keyLength <= 100).length;
      
      const keyLengthImpact = avgTimeForLongKeys - avgTimeForShortKeys;
      
      // Long key performance validation
      expect(avgTimeForLongKeys).toBeLessThan(5); // <5ms even for very long keys
      expect(keyLengthImpact).toBeLessThan(2); // <2ms impact from key length
      expect(maxKeyLength).toBeGreaterThan(1000); // Tested with keys >1000 chars
      
      console.log('‚úÖ Long Key Performance Results:', {
        maxKeyLength,
        avgTimeForShortKeys: `${avgTimeForShortKeys.toFixed(3)}ms`,
        avgTimeForLongKeys: `${avgTimeForLongKeys.toFixed(3)}ms`,
        keyLengthImpact: `${keyLengthImpact.toFixed(3)}ms`
      });
    });

    it('should maintain performance with rapid key creation and deletion', async () => {
      console.log('‚ö° Testing performance with rapid key churn...');
      
      const keyChurnResults: Array<{
        operation: 'create' | 'delete';
        keyId: number;
        responseTime: number;
        success: boolean;
      }> = [];

      const churnLimiter = new RateLimiter({
        windowMs: 60000,
        maxRequests: 1000, // High limit to avoid blocking
      });

      // Rapidly create and delete keys
      for (let cycle = 0; cycle < 100; cycle++) {
        // Create 10 keys
        for (let i = 0; i < 10; i++) {
          const keyId = cycle * 10 + i;
          const testKey = `churn-key-${keyId}`;
          
          const createStart = performance.now();
          const result = await churnLimiter.checkLimit(testKey);
          const createEnd = performance.now();
          
          keyChurnResults.push({
            operation: 'create',
            keyId,
            responseTime: createEnd - createStart,
            success: result.success
          });
        }
        
        // Simulate deletion by reset (in real implementation)
        for (let i = 0; i < 10; i++) {
          const keyId = cycle * 10 + i;
          const testKey = `churn-key-${keyId}`;
          
          const deleteStart = performance.now();
          churnLimiter.reset(testKey);
          const deleteEnd = performance.now();
          
          keyChurnResults.push({
            operation: 'delete',
            keyId,
            responseTime: deleteEnd - deleteStart,
            success: true
          });
        }
      }

      // Analyze key churn performance
      const createOperations = keyChurnResults.filter(r => r.operation === 'create');
      const deleteOperations = keyChurnResults.filter(r => r.operation === 'delete');
      
      const avgCreateTime = createOperations.reduce((sum, r) => sum + r.responseTime, 0) / createOperations.length;
      const avgDeleteTime = deleteOperations.reduce((sum, r) => sum + r.responseTime, 0) / deleteOperations.length;
      
      const totalOperations = keyChurnResults.length;
      const totalTime = keyChurnResults.reduce((sum, r) => sum + r.responseTime, 0);
      const operationsPerSecond = totalOperations / (totalTime / 1000);
      
      // Key churn performance validation
      expect(avgCreateTime).toBeLessThan(5); // <5ms key creation
      expect(avgDeleteTime).toBeLessThan(2); // <2ms key deletion
      expect(operationsPerSecond).toBeGreaterThan(1000); // >1000 operations/sec
      
      console.log('‚úÖ Key Churn Performance Results:', {
        totalOperations,
        avgCreateTime: `${avgCreateTime.toFixed(3)}ms`,
        avgDeleteTime: `${avgDeleteTime.toFixed(3)}ms`,
        operationsPerSecond: `${operationsPerSecond.toFixed(2)} ops/sec`
      });
    });
  });

  afterAll(() => {
    console.log('\n‚ö° WS-199 Performance Benchmarking Completed!');
    console.log('‚úÖ Core Performance (<5ms): VALIDATED');
    console.log('‚úÖ Concurrent Access Performance: VALIDATED');
    console.log('‚úÖ Wedding Industry Specific Performance: VALIDATED');
    console.log('‚úÖ System Resource Efficiency: VALIDATED');
    console.log('‚úÖ Edge Case Performance: VALIDATED');
    console.log('üèÜ All Performance Requirements Met!');
    console.log('üìä Ready for Wedding Peak Season Traffic');
  });
});